[
["index.html", "Analysis of single cell RNA-seq data: 2018 BioInfoSummer Workshop 1 Welcome! 1.1 Preface 1.2 About the course 1.3 GitHub 1.4 Installation 1.5 License 1.6 Prerequisites 1.7 Contact", " Analysis of single cell RNA-seq data: 2018 BioInfoSummer Workshop Stephanie Hicks (stephaniehicks) 2018-12-03 1 Welcome! 1.1 Preface The material for this work was kindly borrowed with permission and adapted from the fantastic online course Analysis of single cell RNA-seq data from Vladimir Kiselev (wikiselev), Tallulah Andrews (talandrews), Jennifer Westoby (Jenni_Westoby), Davis McCarthy (davisjmcc), Maren Büttner (marenbuettner) and Martin Hemberg (m_hemberg). The material in the course above covers about 1.5 days and we will be taking a subset of the material for our 2-3 hour workshop for 2018 BioInfoSummer. 1.2 About the course Today it is possible to obtain genome-wide transcriptome data from single cells using high-throughput sequencing (scRNA-seq). The main advantage of scRNA-seq is that the cellular resolution and the genome wide scope makes it possible to address issues that are intractable using other methods, e.g. bulk RNA-seq or single-cell RT-qPCR. However, to analyze scRNA-seq data, novel methods are required and some of the underlying assumptions for the methods developed for bulk RNA-seq experiments are no longer valid. In this course we will discuss some of the questions that can be addressed using scRNA-seq as well as the available computational and statistical methods available. The number of computational tools is increasing rapidly and we are doing our best to keep up to date with what is available. One of the main constraints for this course is that we would like to use tools that are implemented in R and that run reasonably fast. Moreover, we will also confess to being somewhat biased towards methods that have been developed either by us or by our friends and colleagues. 1.3 GitHub The orginal and complete course material is available at: https://github.com/hemberg-lab/scRNA.seq.course The adapted material for this course at BioInfoSummer 2018 is available at: https://github.com/stephaniehicks/2018-bioinfosummer-scrnaseq 1.4 Installation The course material is available on the course GitHub repository which can be cloned using git clone https://github.com/stephaniehicks/2018-bioinfosummer-scrnaseq 1.5 License The license from the original course material is licensed under GPL-3 and that license is maintained here. Anyone is welcome to go through the material in order to learn about analysis of scRNA-seq data. If you plan to use the material for your own teaching, the original authors have requested that they would appreciate it if you tell them about it in addition to providing a suitable citation. Please contact the original lead author Vladimir Kiselev. 1.6 Prerequisites The course is intended for those who have basic familiarity with Unix and the R scripting language. We will also assume that you are familiar with mapping and analysing bulk RNA-seq data as well as with the commonly available computational tools. 1.7 Contact If you have any comments, questions or suggestions about the original and complete course material, please contact Vladimir Kiselev. If you have questions about the material presented in this course at BioInfoSummer 2018, you can reach me at Stephanie Hicks "],
["introduction-to-single-cell-rna-seq.html", "2 Introduction to single-cell RNA-seq 2.1 Bulk RNA-seq 2.2 scRNA-seq 2.3 Workflow 2.4 Computational Analysis 2.5 Challenges 2.6 Experimental methods 2.7 What platform to use for my experiment?", " 2 Introduction to single-cell RNA-seq 2.1 Bulk RNA-seq A major breakthrough (replaced microarrays) in the late 2000’s and has been widely used since Measures the average expression level for each gene across a large population of input cells Useful for comparative transcriptomics, e.g. samples of the same tissue from different species Useful for quantifying expression signatures from ensembles, e.g. in disease studies Insufficient for studying heterogeneous systems, e.g. early development studies, complex tissues (brain) Does not provide insights into the stochastic nature of gene expression 2.2 scRNA-seq A new technology, first publication by (Tang et al. 2009) Did not gain widespread popularity until ~2014 when new protocols and lower sequencing costs made it more accessible Measures the distribution of expression levels for each gene across a population of cells Allows to study new biological questions in which cell-specific changes in transcriptome are important, e.g. cell type identification, heterogeneity of cell responses, stochasticity of gene expression, inference of gene regulatory networks across the cells. Datasets range from \\(10^2\\) to \\(10^6\\) cells and increase in size every year Currently there are several different protocols in use, e.g. SMART-seq2 (Picelli et al. 2013), CELL-seq (Hashimshony et al. 2012) and Drop-seq (Macosko et al. 2015) There are also commercial platforms available, including the Fluidigm C1, Wafergen ICELL8 and the 10X Genomics Chromium Several computational analysis methods from bulk RNA-seq can be used But in most cases computational analysis requires adaptation of the existing methods or development of new ones 2.3 Workflow Figure 2.1: Single cell sequencing (taken from Wikipedia) Overall, experimental scRNA-seq protocols are similar to the methods used for bulk RNA-seq. We will be discussing some of the most common approaches in the next chapter. 2.4 Computational Analysis This course is concerned with the computational analysis of the data obtained from scRNA-seq experiments. The first steps (yellow) are general for any highthroughput sequencing data. Later steps (orange) require a mix of existing RNASeq analysis methods and novel methods to address the technical difference of scRNASeq. Finally the biological interpretation (blue) should be analyzed with methods specifically developed for scRNASeq. Figure 2.2: Flowchart of the scRNA-seq analysis There are several reviews of the scRNA-seq analysis available including (Stegle, Teichmann, and Marioni 2015). Today, there are also several different analysis tools available for carrying out one or more steps in the flowchart above. These include: Bioconductor is a open-source, open-development software project for the analysis of high-throughput genomics data, including packages for the analysis of single-cell data. Seurat is an R package designed for QC, analysis, and exploration of single cell RNA-seq data. scanpy is a Python-based, scalable toolkit for analyzing single-cell gene expression data. It includes preprocessing, visualization, clustering, pseudotime and trajectory inference and differential expression testing. Falco a single-cell RNA-seq processing framework on the cloud. ASAP (Automated Single-cell Analysis Pipeline) is an interactive web-based platform for single-cell analysis. 2.5 Challenges The main difference between bulk and single cell RNA-seq is that each sequencing library represents a single cell, instead of a population of cells. Therefore, significant attention has to be paid to comparison of the results from different cells (sequencing libraries). The main sources of discrepancy between the libraries are: Amplification (up to 1 million fold) Gene ‘dropouts’ in which a gene is observed at a moderate expression level in one cell but is not detected in another cell (Kharchenko, Silberstein, and Scadden 2014, @Hicks:2018aa). In both cases the discrepancies are introduced due to low starting amounts of transcripts since the RNA comes from one cell only. Improving the transcript capture efficiency and reducing the amplification bias are currently active areas of research. However, as we shall see in this course, it is possible to alleviate some of these issues through proper normalization and corrections. 2.6 Experimental methods Figure 2.3: Moore’s law in single cell transcriptomics (image taken from Svensson et al) Development of new methods and protocols for scRNA-seq is currently a very active area of research, and several protocols have been published over the last few years. A non-comprehensive list includes: CEL-seq (Hashimshony et al. 2012) CEL-seq2 (Hashimshony et al. 2016) Drop-seq (Macosko et al. 2015) InDrop-seq (Klein et al. 2015) MARS-seq (Jaitin et al. 2014) SCRB-seq (Soumillon et al. 2014) Seq-well (Gierahn et al. 2017) Smart-seq (Picelli et al. 2014) Smart-seq2 (Picelli et al. 2014) SMARTer STRT-seq (Islam et al. 2013) 10X Genomics Chromium (Zheng et al. 2017) The methods can be categorized in different ways, but the two most important aspects are quantification and capture. For quantification, there are two types, full-length and tag-based. The former tries to achieve a uniform read coverage of each transcript. By contrast, tag-based protocols only capture either the 5’- or 3’-end of each RNA. The choice of quantification method has important implications for what types of analyses the data can be used for. In theory, full-length protocols should provide an even coverage of transcripts, but as we shall see, there are often biases in the coverage. The main advantage of tag-based protocol is that they can be combined with unique molecular identifiers (UMIs) which can help improve the quantifications. On the other hand, being restricted to one end of the transcript may reduce the mappability and it also makes it harder to distinguish different isoforms (Archer et al. 2016). The strategy used for capture determines throughput, how the cells can be selected as well as what kind of additional information besides the sequencing that can be obtained. The three most widely used options are microwell-, microfluidic- and droplet- based. Figure 2.4: Image of microwell plates (image taken from Wikipedia) For well-based platforms, cells are isolated using for example pipette or laser capture and placed in microfluidic wells. One advantage of well-based methods is that they can be combined with fluorescent activated cell sorting (FACS), making it possible to select cells based on surface markers. This strategy is thus very useful for situations when one wants to isolate a specific subset of cells for sequencing. Another advantage is that one can take pictures of the cells. The image provides an additional modality and a particularly useful application is to identify wells containg damaged cells or doublets. The main drawback of these methods is that they are often low-throughput and the amount of work required per cell may be considerable. Figure 2.5: Image of a 96-well Fluidigm C1 chip (image taken from Fluidigm) Microfluidic platforms, such as Fluidigm’s C1, provide a more integrated system for capturing cells and for carrying out the reactions necessary for the library preparations. Thus, they provide a higher throughput than microwell based platforms. Typically, only around 10% of cells are captured in a microfluidic platform and thus they are not appropriate if one is dealing with rare cell-types or very small amounts of input. Moreover, the chip is relatively expensive, but since reactions can be carried out in a smaller volume money can be saved on reagents. Figure 2.6: Schematic overview of the drop-seq method (Image taken from Macosko et al) The idea behind droplet-based methods is to encapsulate each individual cell inside a nanoliter droplet together with a bead. The bead is loaded with the enzymes required to construct the library. In particular, each bead contains a unique barcode which is attached to all of the reads originating from that cell. Thus, all of the droplets can be pooled, sequenced together and the reads can subsequently be assigned to the cell of origin based on the barcodes. Droplet platforms typically have the highest throughput since the library preparation costs are on the order of \\(.05\\) USD/cell. Instead, sequencing costs often become the limiting factor and a typical experiment the coverage is low with only a few thousand different transcripts detected (Ziegenhain et al. 2017). 2.7 What platform to use for my experiment? The most suitable platform depends on the biological question at hand. For example, if one is interested in characterizing the composition of a tissue, then a droplet-based method which will allow a very large number of cells to be captured is likely to be the most appropriate. On the other hand, if one is interesting in characterizing a rare cell-population for which there is a known surface marker, then it is probably best to enrich using FACS and then sequence a smaller number of cells. Clearly, full-length transcript quantification will be more appropriate if one is interested in studying different isoforms since tagged protocols are much more limited. By contrast, UMIs can only be used with tagged protocols and they can facilitate gene-level quantification. Two recent studies from the Enard group (Ziegenhain et al. 2017) and the Teichmann group (Svensson et al. 2017) have compared several different protocols. In their study, Ziegenhain et al compared five different protocols on the same sample of mouse embryonic stem cells (mESCs). By controlling for the number of cells as well as the sequencing depth, the authors were able to directly compare the sensitivity, noise-levels and costs of the different protocols. One example of their conclusions is illustrated in the figure below which shows the number of genes detected (for a given detection threshold) for the different methods. As you can see, there is almost a two-fold difference between drop-seq and Smart-seq2, suggesting that the choice of protocol can have a major impact on the study Figure 2.7: Enard group study Svensson et al take a different approach by using synthetic transcripts (spike-ins, more about these later) with known concentrations to measure the accuracy and sensitivity of different protocols. Comparing a wide range of studies, they also reported substantial differences between the protocols. Figure 2.8: Teichmann group study As protocols are developed and computational methods for quantifying the technical noise are improved, it is likely that future studies will help us gain further insights regarding the strengths of the different methods. These comparative studies are helpful not only for helping researchers decide which protocol to use, but also for developing new methods as the benchmarking makes it possible to determine what strategies are the most useful ones. References "],
["construction-of-expression-matrix.html", "3 Construction of expression matrix 3.1 Raw sequencing reads QC 3.2 Reads alignment 3.3 Reads quantification 3.4 Unique Molecular Identifiers (UMIs)", " 3 Construction of expression matrix Many analyses of scRNA-seq data take as their starting point an expression matrix. By convention, the each row of the expression matrix represents a gene and each column represents a cell (although some authors use the transpose). Each entry represents the expression level of a particular gene in a given cell. The units by which the expression is meassured depends on the protocol and the normalization strategy used. In this section, we will describe how to go from a set of raw sequencing reads to an expression matrix. 3.1 Raw sequencing reads QC 3.1.1 FastQ The output from a scRNA-seq experiment is a large collection of cDNA reads. FastQ is the most raw form of scRNA-seq data you will encounter. The first step is to ensure that the reads are of high quality. All scRNA-seq protocols are sequenced with paired-end sequencing. Barcode sequences may occur in one or both reads depending on the protocol employed. However, protocols using unique molecular identifiers (UMIs) will generally contain one read with the cell and UMI barcodes plus adapters but without any transcript sequence. Thus reads will be mapped as if they are single-end sequenced despite actually being paired end. 3.1.2 Check the quality of the raw sequencing reads FastQC is a good tool for this. This software produces a FastQC report than be used to evaluate questions such as How good quality are the reads? Is there anything we should be concerned about?, etc. 3.1.3 Trim sequencing adapaters from sequencing reads For example, Trim Galore! is wrapper for trimming reads from FastQ files using cutadapt. Once you trim the adapaters, you can re-run FastQC to confirm successful removal. 3.2 Reads alignment After trimming low quality bases from the reads, the remaining sequences can be mapped to a reference genome. Again, there is no need for a special purpose method for this, so we can use the STAR or the TopHat aligner. For large full-transcript datasets from well annotated organisms (e.g. mouse, human) pseudo-alignment methods (e.g. Kallisto, Salmon) may out-perform conventional alignment. For drop-seq based datasets with tens- or hundreds of thousands of reads pseudoaligners become more appealing since their run-time can be several orders of magnitude less than traditional aligners. 3.2.1 Genome (FASTA, GTF) To map your reads you will need the reference genome and in many cases the genome annotation file (in either GTF or GFF format). These can be downloaded for model organisms from any of the main genomics databases: Ensembl, NCBI, or UCSC Genome Browser. GTF files contain annotations of genes, transcripts, and exons. They must contain: seqname : chromosome/scaffold source : where this annotation came from feature : what kind of feature is this? (e.g. gene, transcript, exon) start : start position (bp) end : end position (bp) score : a number strand : + (forward) or - (reverse) frame : if CDS indicates which base is the first base of the first codon (0 = first base, 1 = second base, etc..) attribute : semicolon-separated list of tag-value pairs of extra information (e.g. names/IDs, biotype) Empty fields are marked with “.” In our experience: Ensembl is the easiest of these to use, and has the largest set of annotations NCBI tends to be more strict in including only high confidence gene annotations Whereas UCSC contains multiple geneset annotations that use different criteria If your experimental system includes non-standard sequences these must be added to both the genome fasta and gtf to quantify their expression. Most commonly this is done for the ERCC spike-ins, although the same must be done for CRISPR- related sequences or other overexpression/reporter constructs. 3.2.2 Alignment using Star An example of how to map reads.fq (the .gz means the file is zipped) using STAR is $&lt;path_to_STAR&gt;/STAR --runThreadN 1 --runMode alignReads --readFilesIn reads1.fq.gz reads2.fq.gz --readFilesCommand zcat --genomeDir &lt;path&gt; --parametersFiles FileOfMoreParameters.txt --outFileNamePrefix &lt;outpath&gt;/output Note, if the spike-ins are used, the reference sequence should be augmented with the DNA sequence of the spike-in molecules prior to mapping. Note, when UMIs are used, their barcodes should be removed from the read sequence. A common practice is to add the barcode to the read name. Once the reads for each cell have been mapped to the reference genome, we need to make sure that a sufficient number of reads from each cell could be mapped to the reference genome. In our experience, the fraction of mappable reads for mouse or human cells is 60-70%. However, this result may vary depending on protocol, read length and settings for the read alignment. As a general rule, we expect all cells to have a similar fraction of mapped reads, so any outliers should be inspected and possibly removed. A low proportion of mappable reads usually indicates contamination. 3.2.3 Pseudo-alignment using Salmon An example of how to quantify expression using Salmon is $&lt;path_to_Salmon&gt;/salmon quant -i salmon_transcript_index -1 reads1.fq.gz -2 reads2.fq.gz -p #threads -l A -g genome.gtf --seqBias --gcBias --posBias Note Salmon produces estimated read counts and estimated transcripts per million (tpm) in our experience the latter over corrects the expression of long genes for scRNA-seq, thus we recommend using read counts. 3.2.4 Mapped reads: BAM file format BAM file format stores mapped reads in a standard and efficient manner. The human-readable version is called a SAM file, while the BAM file is the highly compressed version. BAM/SAM files contain a header which typically includes information on the sample preparation, sequencing and mapping; and a tab-separated row for each individual alignment of each read. BAM/SAM files can be converted to the other format using ‘samtools’: 3.2.4.1 Checking read quality from BAM file Assuming that our reads are in experiment.bam, we can run FastQC as &lt;path_to_fastQC&gt;/fastQC experiment.bam Below is an example of the output from FastQC for a dataset of 125 bp reads. The plot reveals a technical error which resulted in a couple of bases failing to be read correctly in the centre of the read. However, since the rest of the read was of high quality this error will most likely have a negligible effect on mapping efficiency. Figure 3.1: Example of FastQC output Additionally, it is often helpful to visualize the data using the Integrative Genomics Browser (IGV) or SeqMonk tools. 3.2.5 CRAM CRAM files are similar to BAM files only they contain information in the header to the reference genome used in the mapping in the header. This allow the bases in each read that are identical to the reference to be further compressed. CRAM also supports some data compression approaches to further optimize storage compared to BAMs. CRAMs are mainly used by the Sanger/EBI sequencing facility. 3.2.6 Reads Mapping QC 3.2.6.1 Total number of reads mapped to each cell The histogram below shows the total number of reads mapped to each cell for an scRNA-seq experiment. Each bar represents one cell, and they have been sorted in ascending order by the total number of reads per cell. The three red arrows indicate cells that are outliers in terms of their coverage and they should be removed from further analysis. The two yellow arrows point to cells with a surprisingly large number of unmapped reads. In this example we kept the cells during the alignment QC step, but they were later removed during cell QC due to a high proportion of ribosomal RNA reads. Figure 3.2: Example of the total number of reads mapped to each cell. 3.2.6.2 Mapping quality After mapping the raw sequencing to the genome we need to evaluate the quality of the mapping. There are many ways to measure the mapping quality, including: amount of reads mapping to rRNA/tRNAs, proportion of uniquely mapping reads, reads mapping across splice junctions, read depth along the transcripts. Methods developed for bulk RNA-seq, such as RSeQC, are applicable to single-cell data. However the expected results will depend on the experimental protocol, e.g. many scRNA-seq methods use poly-A selection to avoid sequencing rRNAs which results in a 3’ bias in the read coverage across the genes (aka gene body coverage). The figure below shows this 3’ bias as well as three cells which were outliers and removed from the dataset: Figure 3.3: Example of the 3’ bias in the read coverage. 3.3 Reads quantification The next step is to quantify the expression level of each gene for each cell. For mRNA data, we can use one of the tools which has been developed for bulk RNA-seq data, e.g. HT-seq or FeatureCounts (can read about parameters in user guide here). # include multimapping &lt;featureCounts_path&gt;/featureCounts -O -M -Q 30 -p -a genome.gtf -o outputfile input.bam # exclude multimapping &lt;featureCounts_path&gt;/featureCounts -Q 30 -p -a genome.gtf -o outputfile input.bam Unique molecular identifiers (UMIs) make it possible to count the absolute number of molecules and they have proven popular for scRNA-seq. 3.4 Unique Molecular Identifiers (UMIs) Unique Molecular Identifiers are short (4-10bp) random barcodes added to transcripts during reverse-transcription. They enable sequencing reads to be assigned to individual transcript molecules and thus the removal of amplification noise and biases from scRNASeq data. Figure 3.4: UMI sequencing protocol When sequencing UMI containing data, techniques are used to specifically sequence only the end of the transcript containing the UMI (usually the 3’ end). For more information processing reads from a UMI experiment, including mapping and counting barcodes, I refer you to the UMI chapter in original course material. However, determining how to best process and use UMIs is currently an active area of research in the bioinformatics community. We are aware of several methods that have recently been developed, including: UMI-tools PoissonUMIs zUMIs dropEst "],
["introduction-to-rbioconductor.html", "4 Introduction to R/Bioconductor 4.1 Installing R packages 4.2 Data Types 4.3 Bioconductor 4.4 Data visualization using ggplot2", " 4 Introduction to R/Bioconductor 4.1 Installing R packages 4.1.1 CRAN The Comprehensive R Archive Network CRAN is the biggest archive of R packages. There are few requirements for uploading packages besides building and installing succesfully, hence documentation and support is often minimal and figuring how to use these packages can be a challenge it itself. CRAN is the default repository R will search to find packages to install: install.packages(&quot;RColorBrewer&quot;, &quot;reshape2&quot;, &quot;matrixStats&quot;, &quot;mclust&quot;, &quot;pheatmap&quot;, &quot;mvoutlier&quot;) library(matrixStats) 4.1.2 Github Github isn’t specific to R, any code of any type in any state can be uploaded. There is no guarantee a package uploaded to github will even install, nevermind do what it claims to do. R packages can be downloaded and installed directly from github using the devtools package installed above. install.packages(&quot;devtools&quot;) library(&quot;devtools&quot;) devtools::install_github(&quot;hemberg-lab/scRNA.seq.funcs&quot;) devtools::install_github(&quot;theislab/kBET&quot;) Github is also a version control system which stores multiple versions of any package. By default the most recent “master” version of the package is installed. If you want an older version or the development branch this can be specified using the “ref” parameter: # different branch devtools::install_github(&quot;path/to/github/repo&quot;, ref=&quot;older-branch&quot;) # previous commit devtools::install_github(&quot;path/to/github/repo&quot;, ref=&quot;previous-commit-id&quot;) 4.1.3 Bioconductor Bioconductor is an open-source, open-development repository of R-packages specifically for high-throughput biological analyses. It has the strictest requirements for submission, including installation on every platform and full documentation with a tutorial (called a vignette) explaining how the package should be used. Bioconductor also encourages utilization of standard data structures/classes and coding style/naming conventions, so that, in theory, packages and analyses can be combined into large pipelines or workflows. This is the old way of installing packages: source(&quot;https://bioconductor.org/biocLite.R&quot;) biocLite(&quot;edgeR&quot;) Note: in some situations it is necessary to substitute “http://” for “https://” in the above depending on the security features of your internet connection/network. However, this is the new (and improved way) of installing Bioconductor packages using BiocManager: install.packages(&quot;BiocManager&quot;) library(&quot;BiocManager&quot;) BiocManager::install(&quot;scran&quot;, &quot;Rtsne&quot;, &quot;sva&quot;, &quot;DESeq2&quot;, &quot;edgeR&quot;, &quot;SC3&quot;, &quot;zinbwave&quot;) You can even install packages from a specific version of Bioconductor: BiocManager::install(&quot;scater&quot;, version=&quot;devel&quot;) # development branch BiocManager::install(&quot;scater&quot;, version=&quot;3.8&quot;) # current release as of Fall 2018 Bioconductor also requires creators to support their packages and has a regular 6-month release schedule. Make sure you are using the most recent release of bioconductor before trying to install packages for the course. BiocManager::install() 4.1.4 Source The final way to install packages is directly from source. In this case you have to download a fully built source code file, usually packagename.tar.gz, or clone the github repository and rebuild the package yourself. Generally this will only be done if you want to edit a package yourself, or if for some reason the former methods have failed. pkgs &lt;- rownames(installed.packages()) BiocManager::install(pkgs, type = &quot;source&quot;) 4.2 Data Types 4.2.1 What is Tidy Data? The tidyverse is “an opinionated collection of R packages designed for data science. All packages share an underlying philosophy and common APIs.” Another way of putting it is that it’s a set of packages that are useful specifically for data manipulation, exploration and visualization with a common philosphy. 4.2.1.1 What is this common philosphy? The common philosphy is called “tidy” data. It is a standard way of mapping the meaning of a dataset to its structure. In tidy data: Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. Below, we are interested in transformating the table on the right to the the table on the left, which is considered “tidy”. Working with tidy data is useful because it creates a structured way of organizing data values within a data set. This makes the data analysis process more efficient and simplifies the development of data analysis tools that work together. In this way, you can focus on the problem you are investigating, rather than the uninteresting logistics of data. 4.2.1.2 What is in the tidyverse? We can install and load the set of R packages using install.packages(&quot;tidyverse&quot;) function. When we load the tidyverse package using library(tidyverse), there are six core R packages that load: readr, for data import. tidyr, for data tidying. dplyr, for data wrangling. ggplot2, for data visualisation. purrr, for functional programming. tibble, for tibbles, a modern re-imagining of data frames. Here, we load in the tidyverse. library(tidyverse) These packages are highlighted in bold here: Because these packages all share the “tidy” philosphy, the data analysis workflow is easier as you move from package to package. If we are going to take advantage of the “tidyverse”, this means we need to transform the data into a form that is “tidy”. If you recall, in tidy data: Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. For example, consider the following dataset: Here: each row represents one company (row names are companies) each column represent one time point the stock prices are defined for each row/column pair Alternatively, a data set can be structured in the following way: each row represents one time point (but no row names) the first column defines the time variable and the last three columns contain the stock prices for three companies In both cases, the data is the same, but the structure is different. This can be frustrating to deal with as an analyst because the meaning of the values (rows and columns) in the two data sets are different. Providing a standardized way of organizing values within a data set would alleviate a major portion of this frustration. For motivation, a tidy version of the stock data we looked at above looks like this: (we’ll learn how the functions work in just a moment) In this “tidy” data set, we have three columns representing three variables (time, company name and stock price). Every row represents contains one stock price from a particular time and for a specific company. The tidyr is an R package that transforms data sets to a tidy format. 4.2.2 What is Rich Data? If you google ‘rich data’, you will find lots of different definitions for this term. In this course, we will use ‘rich data’ to mean data which is generated by combining information from multiple sources. For example, you could make rich data by creating an object in R which contains a matrix of gene expression values across the cells in your scRNA-seq experiment, but also information about how the experiment was performed. Objects of the SingleCellExperiment class, which we will discuss below, are an example of rich data. 4.3 Bioconductor From Wikipedia: Bioconductor is a free, open source and open development software project for the analysis and comprehension of genomic data generated by wet lab experiments in molecular biology. Bioconductor is based primarily on the statistical R programming language, but does contain contributions in other programming languages. It has two releases each year that follow the semiannual releases of R. At any one time there is a release version, which corresponds to the released version of R, and a development version, which corresponds to the development version of R. Most users will find the release version appropriate for their needs. We strongly recommend all new comers and even experienced high-throughput data analysts to use well developed and maintained Bioconductor methods and classes. 4.3.1 SingleCellExperiment class SingleCellExperiment (SCE) is a S4 class for storing data from single-cell experiments. This includes specialized methods to store and retrieve spike-in information, dimensionality reduction coordinates and size factors for each cell, along with the usual metadata for genes and libraries. In practice, an object of this class can be created using the SingleCellExperiment() constructor: library(SingleCellExperiment) counts &lt;- matrix(rpois(100, lambda = 10), ncol=10, nrow=10) rownames(counts) &lt;- paste(&quot;gene&quot;, 1:10, sep = &quot;&quot;) colnames(counts) &lt;- paste(&quot;cell&quot;, 1:10, sep = &quot;&quot;) sce &lt;- SingleCellExperiment( assays = list(counts = counts), rowData = data.frame(gene_names = paste(&quot;gene_name&quot;, 1:10, sep = &quot;&quot;)), colData = data.frame(cell_names = paste(&quot;cell_name&quot;, 1:10, sep = &quot;&quot;)) ) sce ## class: SingleCellExperiment ## dim: 10 10 ## metadata(0): ## assays(1): counts ## rownames(10): gene1 gene2 ... gene9 gene10 ## rowData names(1): gene_names ## colnames(10): cell1 cell2 ... cell9 cell10 ## colData names(1): cell_names ## reducedDimNames(0): ## spikeNames(0): In the SingleCellExperiment object, users can assign arbitrary names to entries of assays. To assist interoperability between packages, some suggestions for what the names should be for particular types of data are provided by the authors: counts: Raw count data, e.g., number of reads or transcripts for a particular gene. normcounts: Normalized values on the same scale as the original counts. For example, counts divided by cell-specific size factors that are centred at unity. logcounts: Log-transformed counts or count-like values. In most cases, this will be defined as log-transformed normcounts, e.g., using log base 2 and a pseudo-count of 1. cpm: Counts-per-million. This is the read count for each gene in each cell, divided by the library size of each cell in millions. tpm: Transcripts-per-million. This is the number of transcripts for each gene in each cell, divided by the total number of transcripts in that cell (in millions). Each of these suggested names has an appropriate getter/setter method for convenient manipulation of the SingleCellExperiment. For example, we can take the (very specifically named) counts slot, normalize it and assign it to normcounts instead: normcounts(sce) &lt;- log2(counts(sce) + 1) sce ## class: SingleCellExperiment ## dim: 10 10 ## metadata(0): ## assays(2): counts normcounts ## rownames(10): gene1 gene2 ... gene9 gene10 ## rowData names(1): gene_names ## colnames(10): cell1 cell2 ... cell9 cell10 ## colData names(1): cell_names ## reducedDimNames(0): ## spikeNames(0): dim(normcounts(sce)) ## [1] 10 10 head(normcounts(sce)) ## cell1 cell2 cell3 cell4 cell5 cell6 cell7 ## gene1 3.807355 3.584963 3.000000 3.584963 3.169925 3.321928 4.247928 ## gene2 3.700440 3.807355 3.000000 3.321928 2.584963 3.459432 3.807355 ## gene3 3.807355 3.321928 3.584963 3.169925 3.584963 3.584963 3.459432 ## gene4 3.169925 3.584963 3.584963 3.321928 3.584963 3.459432 3.459432 ## gene5 3.584963 3.321928 3.807355 3.000000 3.321928 3.584963 3.459432 ## gene6 3.169925 3.169925 3.584963 3.584963 3.000000 3.000000 3.169925 ## cell8 cell9 cell10 ## gene1 3.807355 3.700440 2.321928 ## gene2 4.087463 3.584963 3.906891 ## gene3 2.321928 3.584963 3.906891 ## gene4 3.169925 3.169925 2.321928 ## gene5 3.459432 3.321928 3.000000 ## gene6 3.169925 4.000000 3.906891 4.3.2 scater package library(scater) scater is a R package for single-cell RNA-seq analysis (McCarthy et al. 2017). The package contains several useful methods for quality control, visualisation and pre-processing of data prior to further downstream analysis. scater features the following functionality: Automated computation of QC metrics Transcript quantification from read data with pseudo-alignment Data format standardisation Rich visualizations for exploratory analysis Seamless integration into the Bioconductor universe Simple normalisation methods We highly recommend to use scater for all single-cell RNA-seq analyses and scater is the basis of the first part of the course. As illustrated in the figure below, scater will help you with quality control, filtering and normalization of your expression matrix following mapping and alignment. Keep in mind that this figure represents the original version of scater where an SCESet class was used. In the newest version this figure is still correct, except that SCESet can be substituted with the SingleCellExperiment class. 4.4 Data visualization using ggplot2 4.4.1 What is ggplot2? ggplot2 is an R package designed by Hadley Wickham which facilitates data visualizations. In this section, we will touch briefly on some of the features of the package. If you would like to learn more about how to use ggplot2, we would recommend reading “ggplot2 Elegant graphics for data analysis”, by Hadley Wickham. 4.4.2 Principles of ggplot2 Your data must be a data.frame if you want to plot it using ggplot2. Use the aes mapping function to specify how variables in the data.frame map to features on your plot Use geoms to specify how your data should be represented on your graph eg. as a scatterplot, a barplot, a boxplot etc. 4.4.3 Using the aes() mapping function The aes function specifies how variables in your dataframe map to features on your plot. To understand how this works, let’s look at an example: library(tidyverse) # this loads the ggplot2 R package set.seed(12345) counts &lt;- as.data.frame(matrix(rpois(100, lambda = 10), ncol=10, nrow=10)) gene_ids &lt;- paste(&quot;gene&quot;, 1:10, sep = &quot;&quot;) colnames(counts) &lt;- paste(&quot;cell&quot;, 1:10, sep = &quot;&quot;) counts &lt;- data.frame(gene_ids, counts) counts ## gene_ids cell1 cell2 cell3 cell4 cell5 cell6 cell7 cell8 cell9 cell10 ## 1 gene1 11 9 15 11 12 16 9 7 13 14 ## 2 gene2 12 10 8 8 9 12 8 15 8 8 ## 3 gene3 9 9 11 4 10 7 10 12 2 11 ## 4 gene4 8 11 11 11 10 4 8 9 7 6 ## 5 gene5 11 13 9 6 14 11 12 11 13 10 ## 6 gene6 4 10 12 18 13 13 9 12 10 11 ## 7 gene7 11 12 16 10 7 7 10 6 12 9 ## 8 gene8 7 14 16 5 12 10 10 7 5 10 ## 9 gene9 8 7 15 5 12 7 12 8 8 10 ## 10 gene10 11 4 10 10 16 15 9 6 9 11 ggplot(data = counts, mapping = aes(x = cell1, y = cell2)) Let’s take a closer look at the final command, ggplot(data = counts, mapping = aes(x = cell1, y = cell2)). ggplot() initializes a ggplot object and takes the arguments data and mapping. We pass our dataframe of counts to data and use the aes() function to specify that we would like to use the variable cell1 as our x variable and the variable cell2 as our y variable. Clearly, the plot we have just created is not very informative because no data is displayed on them. To display data, we will need to use geoms. 4.4.4 Geoms We can use geoms to specify how we would like data to be displayed on our graphs. For example, our choice of geom could specify that we would like our data to be displayed as a scatterplot, a barplot or a boxplot. Let’s see how our graph would look as a scatterplot. ggplot(data = counts, mapping = aes(x = cell1, y = cell2)) + geom_point() Now we can see that there doesn’t seem to be any correlation between gene expression in cell1 and cell2. Given we generated counts randomly, this isn’t too surprising. We can modify the command above to create a line plot using the geom_line() function. ggplot(data = counts, mapping = aes(x = cell1, y = cell2)) + geom_line() Hint: execute ?ggplot and scroll down the help page. At the bottom is a link to the ggplot2 package index. Scroll through the index until you find the geom options. 4.4.5 Plotting data from more than 2 cells So far we’ve been considering the gene counts from 2 of the cells in our dataframe. But there are actually 10 cells in our dataframe and it would be nice to compare all of them. What if we wanted to plot data from all 10 cells at the same time? At the moment we can’t do this because we are treating each individual cell as a variable and assigning that variable to either the x or the y axis. We could create a 10 dimensional graph to plot data from all 10 cells on, but this is a) not possible to do with ggplot and b) not very easy to interpret. What we could do instead is to tidy our data so that we had one variable representing cell ID and another variable representing gene counts, and plot those against each other. In code, this would look like: counts &lt;- gather(counts, colnames(counts)[2:11], key = &#39;Cell_ID&#39;, value=&#39;Counts&#39;) head(counts) ## gene_ids Cell_ID Counts ## 1 gene1 cell1 11 ## 2 gene2 cell1 12 ## 3 gene3 cell1 9 ## 4 gene4 cell1 8 ## 5 gene5 cell1 11 ## 6 gene6 cell1 4 Essentially, the problem before was that our data was not tidy because one variable (Cell_ID) was spread over multiple columns. Now that we’ve fixed this problem, it is much easier for us to plot data from all 10 cells on one graph. ggplot(counts,aes(x=Cell_ID, y=Counts)) + geom_boxplot() References "],
["cleaning-the-expression-matrix.html", "5 Cleaning the expression matrix 5.1 Expression QC 5.2 Data visualization 5.3 Identifying confounding factors 5.4 Normalization theory 5.5 Normalization practice 5.6 Dealing with confounders", " 5 Cleaning the expression matrix 5.1 Expression QC 5.1.1 Introduction Once gene expression has been quantified it is refer to it as an expression matrix where each row corresponds to a gene (or transcript) and each column corresponds to a single cell. This matrix should be examined to remove poor quality cells which were not detected in either read QC or mapping QC steps. Failure to remove low quality cells at this stage may add technical noise which has the potential to obscure the biological signals of interest in the downstream analysis. Since there is currently no standard method for performing scRNA-seq the expected values for the various QC measures that will be presented here can vary substantially from experiment to experiment. Thus, to perform QC we will be looking for cells which are outliers with respect to the rest of the dataset rather than comparing to independent quality standards. Consequently, care should be taken when comparing quality metrics across datasets collected using different protocols. 5.1.2 Tung dataset (UMI) To illustrate cell QC, we consider a dataset of induced pluripotent stem cells generated from three different individuals (Tung et al. 2017) in Yoav Gilad’s lab at the University of Chicago. The experiments were carried out on the Fluidigm C1 platform and to facilitate the quantification both unique molecular identifiers (UMIs) and ERCC spike-ins were used. The data files are located in the data/tung folder in your working directory. These files are the copies of the original files made on the 15/03/16. We will use these copies for reproducibility purposes. library(SingleCellExperiment) library(scater) options(stringsAsFactors = FALSE) Load the data and annotations: molecules &lt;- read.table(&quot;data/tung/molecules.txt&quot;, sep = &quot;\\t&quot;) anno &lt;- read.table(&quot;data/tung/annotation.txt&quot;, sep = &quot;\\t&quot;, header = TRUE) Inspect a small portion of the expression matrix head(molecules[, 1:3]) ## NA19098.r1.A01 NA19098.r1.A02 NA19098.r1.A03 ## ENSG00000237683 0 0 0 ## ENSG00000187634 0 0 0 ## ENSG00000188976 3 6 1 ## ENSG00000187961 0 0 0 ## ENSG00000187583 0 0 0 ## ENSG00000187642 0 0 0 head(anno) ## individual replicate well batch sample_id ## 1 NA19098 r1 A01 NA19098.r1 NA19098.r1.A01 ## 2 NA19098 r1 A02 NA19098.r1 NA19098.r1.A02 ## 3 NA19098 r1 A03 NA19098.r1 NA19098.r1.A03 ## 4 NA19098 r1 A04 NA19098.r1 NA19098.r1.A04 ## 5 NA19098 r1 A05 NA19098.r1 NA19098.r1.A05 ## 6 NA19098 r1 A06 NA19098.r1 NA19098.r1.A06 The data consists of 3 individuals and 3 replicates and therefore has 9 batches in total. We standardize the analysis by using both SingleCellExperiment (SCE) and scater packages. First, create the SCE object: umi &lt;- SingleCellExperiment( assays = list(counts = as.matrix(molecules)), colData = anno ) umi ## class: SingleCellExperiment ## dim: 19027 864 ## metadata(0): ## assays(1): counts ## rownames(19027): ENSG00000237683 ENSG00000187634 ... ERCC-00170 ## ERCC-00171 ## rowData names(0): ## colnames(864): NA19098.r1.A01 NA19098.r1.A02 ... NA19239.r3.H11 ## NA19239.r3.H12 ## colData names(5): individual replicate well batch sample_id ## reducedDimNames(0): ## spikeNames(0): Remove genes that are not expressed in any cell: keep_feature &lt;- rowSums(counts(umi) &gt; 0) &gt; 0 umi &lt;- umi[keep_feature, ] Define control features (genes) - ERCC spike-ins and mitochondrial genes (provided by the authors): isSpike(umi, &quot;ERCC&quot;) &lt;- grepl(&quot;^ERCC-&quot;, rownames(umi)) isSpike(umi, &quot;MT&quot;) &lt;- rownames(umi) %in% c(&quot;ENSG00000198899&quot;, &quot;ENSG00000198727&quot;, &quot;ENSG00000198888&quot;, &quot;ENSG00000198886&quot;, &quot;ENSG00000212907&quot;, &quot;ENSG00000198786&quot;, &quot;ENSG00000198695&quot;, &quot;ENSG00000198712&quot;, &quot;ENSG00000198804&quot;, &quot;ENSG00000198763&quot;, &quot;ENSG00000228253&quot;, &quot;ENSG00000198938&quot;, &quot;ENSG00000198840&quot;) Next we use the calculateQCMetrics() function in the scater package to calculate quality control metrics. We refer users to the ?calculateQCMetrics for a full list of the computed cell-level metrics. However, we will describe some of the more popular ones here. For example, some cell-level QC metrics are: total_counts: total number of counts for the cell (i.e., the library size) total_features_by_counts: the number of features for the cell that have counts above the detection limit (default of zero) pct_counts_X: percentage of all counts that come from the feature control set named X Some feature-level metrics include: mean_counts: the mean count of the gene/feature pct_dropout_by_counts: the percentage of cells with counts of zero for each gene pct_counts_Y: percentage of all counts that come from the cell control set named Y Let’s try calculating the quality metrics umi &lt;- calculateQCMetrics( umi, feature_controls = list( ERCC = isSpike(umi, &quot;ERCC&quot;), MT = isSpike(umi, &quot;MT&quot;) ) ) umi ## class: SingleCellExperiment ## dim: 18726 864 ## metadata(0): ## assays(1): counts ## rownames(18726): ENSG00000237683 ENSG00000187634 ... ERCC-00170 ## ERCC-00171 ## rowData names(9): is_feature_control is_feature_control_ERCC ... ## total_counts log10_total_counts ## colnames(864): NA19098.r1.A01 NA19098.r1.A02 ... NA19239.r3.H11 ## NA19239.r3.H12 ## colData names(50): individual replicate ... ## pct_counts_in_top_200_features_MT ## pct_counts_in_top_500_features_MT ## reducedDimNames(0): ## spikeNames(2): ERCC MT If exprs_values is set to something other than “counts”, the names of the metrics will be changed by swapping “counts” for whatever named assay was used. 5.1.3 Cell QC 5.1.3.1 Library size Next we consider the total number of RNA molecules detected per sample (if we were using read counts rather than UMI counts this would be the total number of reads). Wells with few reads/molecules are likely to have been broken or failed to capture a cell, and should thus be removed. hist( umi$total_counts, breaks = 100 ) abline(v = 25000, col = &quot;red&quot;) Figure 5.1: Histogram of library sizes for all cells How many cells does our filter remove? filter_by_total_counts &lt;- (umi$total_counts &gt; 25000) table(filter_by_total_counts) ## filter_by_total_counts ## FALSE TRUE ## 46 818 5.1.3.2 Detected genes In addition to ensuring sufficient sequencing depth for each sample, we also want to make sure that the reads are distributed across the transcriptome. Thus, we count the total number of unique genes detected in each sample. hist( umi$total_features_by_counts, breaks = 100 ) abline(v = 7000, col = &quot;red&quot;) Figure 5.2: Histogram of the number of detected genes in all cells From the plot we conclude that most cells have between 7,000-10,000 detected genes,which is normal for high-depth scRNA-seq. However, this varies byexperimental protocol and sequencing depth. For example, droplet-based methods or samples with lower sequencing-depth typically detect fewer genes per cell. The most notable feature in the above plot is the “heavy tail” on the left hand side of the distribution. If detection rates were equal across the cells then the distribution should be approximately normal. Thus we remove those cells in the tail of the distribution (fewer than 7,000 detected genes). How many cells does our filter remove? filter_by_expr_features &lt;- (umi$total_features_by_counts &gt; 7000) table(filter_by_expr_features) ## filter_by_expr_features ## FALSE TRUE ## 116 748 5.1.3.3 ERCCs and MTs Another measure of cell quality is the ratio between ERCC spike-in RNAs and endogenous RNAs. This ratio can be used to estimate the total amount of RNA in the captured cells. Cells with a high level of spike-in RNAs had low starting amounts of RNA, likely due to the cell being dead or stressed which may result in the RNA being degraded. plotColData( umi, x = &quot;total_features_by_counts&quot;, y = &quot;pct_counts_MT&quot;, colour = &quot;batch&quot; ) Figure 5.3: Percentage of counts in MT genes plotColData( umi, x = &quot;total_features_by_counts&quot;, y = &quot;pct_counts_ERCC&quot;, colour = &quot;batch&quot; ) Figure 5.4: Percentage of counts in ERCCs The above analysis shows that majority of the cells from NA19098.r2 batch have a very high ERCC/Endo ratio. Indeed, it has been shown by the authors that this batch contains cells of smaller size. Let’s create filters for removing batch NA19098.r2 and cells with high expression of mitochondrial genes (&gt;10% of total counts in a cell). filter_by_ERCC &lt;- umi$batch != &quot;NA19098.r2&quot; table(filter_by_ERCC) ## filter_by_ERCC ## FALSE TRUE ## 96 768 filter_by_MT &lt;- umi$pct_counts_MT &lt; 10 table(filter_by_MT) ## filter_by_MT ## FALSE TRUE ## 31 833 5.1.4 Cell filtering 5.1.4.1 Manual Now we can define a cell filter based on our previous analysis: umi$use &lt;- ( # sufficient features (genes) filter_by_expr_features &amp; # sufficient molecules counted filter_by_total_counts &amp; # sufficient endogenous RNA filter_by_ERCC &amp; # remove cells with unusual number of reads in MT genes filter_by_MT ) table(umi$use) ## ## FALSE TRUE ## 207 657 5.1.4.2 Automatic Another option available in scater is to conduct PCA on a set of QC metrics and then use automatic outlier detection to identify potentially problematic cells. By default, the following metrics are used for PCA-based outlier detection: pct_counts_top_100_features total_features pct_counts_feature_controls n_detected_feature_controls log10_counts_endogenous_features log10_counts_feature_controls scater first creates a matrix where the rows represent cells and the columns represent the different QC metrics. Then, outlier cells can also be identified by using the mvoutlier package on the QC metrics for all cells. This will identify cells that have substantially different QC metrics from the others, possibly corresponding to low-quality cells. We can visualize any outliers using a principal components plot as shown below: umi &lt;- runPCA(umi, use_coldata = TRUE, detect_outliers = TRUE) reducedDimNames(umi) ## [1] &quot;PCA_coldata&quot; Column subsetting can then be performed based on the $outlier slot, which indicates whether or not each cell has been designated as an outlier. Automatic outlier detection can be informative, but a close inspection of QC metrics and tailored filtering for the specifics of the dataset at hand is strongly recommended. table(umi$outlier) ## ## FALSE TRUE ## 791 73 Then, we can use a PCA plot to see a 2D representation of the cells ordered by their quality metrics. plotReducedDim(umi, use_dimred = &quot;PCA_coldata&quot;, size_by = &quot;total_features_by_counts&quot;, shape_by = &quot;use&quot;, colour_by=&quot;outlier&quot;) 5.1.5 Compare filterings We can compare the default, automatic and manual cell filters. Plot a Venn diagram of the outlier cells from these filterings. We will use vennCounts() and vennDiagram() functions from the limma package to make a Venn diagram. Figure 5.5: Comparison of the default, automatic and manual cell filters 5.1.6 Gene analysis 5.1.6.1 Gene expression In addition to removing cells with poor quality, it is usually a good idea to exclude genes where we suspect that technical artefacts may have skewed the results. Moreover, inspection of the gene expression profiles may provide insights about how the experimental procedures could be improved. It is often instructive to consider a plot that shows the top 50 (by default) most-expressed features. Each row in the plot below corresponds to a gene, and each bar corresponds to the expression of a gene in a single cell. The circle indicates the median expression of each gene, with which genes are sorted. By default, “expression” is defined using the feature counts (if available), but other expression values can be used instead by changing exprs_values. plotHighestExprs(umi, exprs_values = &quot;counts&quot;) Figure 5.6: Number of total counts consumed by the top 50 expressed genes A few spike-in transcripts may also be present here, though if all of the spike-ins are in the top 50, it suggests that too much spike-in RNA was added and a greater dilution of the spike-ins may be preferrable if the experiment is to be repeated. A large number of pseudo-genes or predicted genes may indicate problems with alignment. 5.1.6.2 Gene filtering It is typically a good idea to remove genes whose expression level is considered “undetectable”. We define a gene as detectable if at least two cells contain more than 1 transcript from the gene. If we were considering read counts rather than UMI counts a reasonable threshold is to require at least five reads in at least two cells. However, in both cases the threshold strongly depends on the sequencing depth. It is important to keep in mind that genes must be filtered after cell filtering since some genes may only be detected in poor quality cells (__note__ colData(umi)$use filter applied to the umi dataset). In the example below, genes are only retained if they are expressed in two or more cells with more than 1 transcript: keep_feature &lt;- nexprs(umi[,colData(umi)$use], byrow=TRUE, detection_limit=1) &gt;= 2 rowData(umi)$use &lt;- keep_feature table(keep_feature) ## keep_feature ## FALSE TRUE ## 4660 14066 Depending on the cell-type, protocol and sequencing depth, other cut-offs may be appropriate. Before we leave this section, we can check the dimensions of the QCed dataset (do not forget about the gene filter we defined above): dim(umi[rowData(umi)$use, colData(umi)$use]) ## [1] 14066 657 Let’s create an additional slot with log-transformed counts (we will need it in the next sections): assay(umi, &quot;logcounts_raw&quot;) &lt;- log2(counts(umi) + 1) 5.2 Data visualization 5.2.1 Introduction In this section, we will continue to work with the filtered Tung dataset produced in the previous section. We will explore different ways of visualizing the data to allow you to asses what happened to the expression matrix after the quality control step. scater package provides several very useful functions to simplify visualisation. One important aspect of single-cell RNA-seq is to control for batch effects. Batch effects are technical artefacts that are added to the samples during handling. For example, if two sets of samples were prepared in different labs or even on different days in the same lab, then we may observe greater similarities between the samples that were handled together. In the worst case scenario, batch effects may be mistaken for true biological variation. The Tung data allows us to explore these issues in a controlled manner since some of the salient aspects of how the samples were handled have been recorded. Ideally, we expect to see batches from the same individual grouping together and distinct groups corresponding to each individual. umi.qc &lt;- umi[rowData(umi)$use, colData(umi)$use] endog_genes &lt;- !rowData(umi.qc)$is_feature_control 5.2.2 PCA plot The easiest way to overview the data is by transforming it using the principal component analysis and then visualize the first two principal components. Principal component analysis (PCA) is a statistical procedure that uses a transformation to convert a set of observations into a set of values of linearly uncorrelated variables called principal components (PCs). The number of principal components is less than or equal to the number of original variables. Mathematically, the PCs correspond to the eigenvectors of the covariance matrix. The eigenvectors are sorted by eigenvalue so that the first principal component accounts for as much of the variability in the data as possible, and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components (the figure below is taken from here). Figure 5.7: Schematic representation of PCA dimensionality reduction 5.2.2.1 Before QC Without log-transformation: plotPCA( umi[endog_genes, ], colour_by = &quot;batch&quot;, size_by = &quot;total_features_by_counts&quot;, shape_by = &quot;individual&quot;, run_args=list(exprs_values=&quot;counts&quot;) ) Figure 5.8: PCA plot of the tung data With log-transformation: plotPCA( umi[endog_genes, ], colour_by = &quot;batch&quot;, size_by = &quot;total_features_by_counts&quot;, shape_by = &quot;individual&quot;, run_args=list(exprs_values=&quot;logcounts_raw&quot;) ) Figure 5.9: PCA plot of the tung data Clearly log-transformation is benefitial for our data. It reduces the variance on the first principal component and already separates some biological effects. Moreover, it makes the distribution of the expression values more normal. In the following analysis and sections we will be using log-transformed raw counts by default. However, note that just a log-transformation is not enough to account for different technical factors between the cells (e.g. sequencing depth). Therefore, please do not use logcounts_raw for your downstream analysis, instead as a minimum suitable data use the logcounts slot of the SingleCellExperiment object, which not just log-transformed, but also normalised by library size (e.g. CPM normalisation). For the moment, we use logcounts_raw only for demonstration purposes! 5.2.2.2 After QC plotPCA( umi.qc[endog_genes, ], colour_by = &quot;batch&quot;, size_by = &quot;total_features_by_counts&quot;, shape_by = &quot;individual&quot;, run_args=list(exprs_values=&quot;logcounts_raw&quot;) ) Figure 5.10: PCA plot of the tung data Comparing Figure 5.9 and Figure 5.10, it is clear that after quality control the NA19098.r2 cells no longer form a group of outliers. By default only the top 500 most variable genes are used by scater to calculate the PCA. This can be adjusted by changing the ntop argument. 5.2.3 tSNE map An alternative to PCA for visualizing scRNASeq data is a tSNE plot. tSNE (t-Distributed Stochastic Neighbor Embedding) combines dimensionality reduction (e.g. PCA) with random walks on the nearest-neighbour network to map high dimensional data (i.e. our 14,214 dimensional expression matrix) to a 2-Dimensional space while preserving local distances between cells. In contrast with PCA, tSNE is a stochastic algorithm which means running the method multiple times on the same dataset will result in different plots. Due to the non-linear and stochastic nature of the algorithm, tSNE is more difficult to intuitively interpret tSNE. To ensure reproducibility, we fix the “seed” of the random-number generator in the code below so that we always get the same plot. 5.2.3.1 Before QC set.seed(123456) plotTSNE( umi[endog_genes, ], colour_by = &quot;batch&quot;, size_by = &quot;total_features_by_counts&quot;, shape_by = &quot;individual&quot;, run_args=list(exprs_values=&quot;logcounts_raw&quot;, perplexity = 130) ) Figure 5.11: tSNE map of the tung data 5.2.3.2 After QC set.seed(123456) plotTSNE( umi.qc[endog_genes, ], colour_by = &quot;batch&quot;, size_by = &quot;total_features_by_counts&quot;, shape_by = &quot;individual&quot;, run_args=list(exprs_values=&quot;logcounts_raw&quot;, perplexity = 130) ) Figure 5.12: tSNE map of the tung data Interpreting PCA and tSNE plots is often challenging and due to their stochastic and non-linear nature, they are less intuitive. However, in this case it is clear that they provide a similar picture of the data. Comparing Figure 5.11 and 5.12, it is again clear that the samples from NA19098.r2 are no longer outliers after the QC filtering. Furthermore tSNE requires you to provide a value of perplexity which reflects the number of neighbours used to build the nearest-neighbour network; a high value creates a dense network which clumps cells together while a low value makes the network more sparse allowing groups of cells to separate from each other. scater uses a default perplexity of the total number of cells divided by five (rounded down). You can read more about the pitfalls of using tSNE here. As an exercise, I will leave it up to you explore how the tSNE plots change when a perplexity of 10 or 200 is used. How does the choice of perplexity affect the interpretation of the results? 5.3 Identifying confounding factors 5.3.1 Introduction There is a large number of potential confounders, artifacts and biases in sc-RNA-seq data. One of the main challenges in analyzing scRNA-seq data stems from the fact that it is difficult to carry out a true technical replicate (why?) to distinguish biological and technical variability. In the previous sections we considered batch effects and in this section we will continue to explore how experimental artifacts can be identified and removed. We will continue using the scater package since it provides a set of methods specifically for quality control of experimental and explanatory variables. Moreover, we will continue to work with the Blischak data that was used in the previous section. Recall that the umi.qc dataset contains filtered cells and genes. Our next step is to explore technical drivers of variability in the data to inform data normalisation before downstream analysis. 5.3.2 Correlations with PCs Let’s first look again at the PCA plot of the QCed dataset: plotPCA( umi.qc[endog_genes, ], colour_by = &quot;batch&quot;, size_by = &quot;total_features_by_counts&quot;, run_args=list(exprs_values=&quot;logcounts_raw&quot;) ) Figure 5.13: PCA plot of the tung data scater allows one to identify principal components that correlate with experimental and QC variables of interest (it ranks principle components by \\(R^2\\) from a linear model regressing PC value against the variable of interest). Let’s test whether some of the variables correlate with any of the PCs. 5.3.2.1 Detected genes plotExplanatoryPCs( umi.qc[endog_genes, ], variables = &quot;total_features_by_counts&quot;, exprs_values=&quot;logcounts_raw&quot;) Figure 5.14: PC correlation with the number of detected genes Indeed, we can see that PC1 can be almost completely explained by the number of detected genes. In fact, it was also visible on the PCA plot above. This is a well-known issue in scRNA-seq and was described (Hicks et al. 2018) or here. 5.3.3 Explanatory variables scater can also compute the marginal \\(R^2\\) for each variable when fitting a linear model regressing expression values for each gene against just that variable, and display a density plot of the gene-wise marginal \\(R^2\\) values for the variables. plotExplanatoryVariables( umi.qc[endog_genes, ], variables = c( &quot;total_features_by_counts&quot;, &quot;total_counts&quot;, &quot;batch&quot;, &quot;individual&quot;, &quot;pct_counts_ERCC&quot;, &quot;pct_counts_MT&quot; ), exprs_values=&quot;logcounts_raw&quot;) Figure 5.15: Explanatory variables This analysis indicates that the number of detected genes (again) and also the sequencing depth (number of counts) have substantial explanatory power for many genes, so these variables are good candidates for conditioning out in a normalisation step, or including in downstream statistical models. Expression of ERCCs also appears to be an important explanatory variable and one notable feature of the above plot is that batch explains more than individual. 5.4 Normalization theory 5.4.1 Introduction In the previous section, we identified important confounding factors and explanatory variables. scater allows one to account for these variables in subsequent statistical models or to condition them out using normaliseExprs(), if so desired. This can be done by providing a design matrix to normaliseExprs(). We are not covering this topic here, but you can try to do it yourself as an exercise. Instead we will explore how simple size-factor normalisations correcting for library size can remove the effects of some of the confounders and explanatory variables. 5.4.2 Library size Library sizes vary because scRNA-seq data is often sequenced on highly multiplexed platforms the total reads which are derived from each cell may differ substantially. Some quantification methods (eg. Cufflinks, RSEM) incorporated library size when determining gene expression estimates thus do not require this normalization. However, if another quantification method was used then library size must be corrected for by multiplying or dividing each column of the expression matrix by a “normalization factor” which is an estimate of the library size relative to the other cells. Many methods to correct for library size have been developped for bulk RNA-seq and can be equally applied to scRNA-seq (eg. UQ, SF, CPM, RPKM, FPKM, TPM). 5.4.3 Normalisations 5.4.3.1 CPM The simplest way to normalize this data is to convert it to counts per million (__CPM__) by dividing each column by its total then multiplying by 1,000,000. Note that spike-ins should be excluded from the calculation of total expression in order to correct for total cell RNA content, therefore we will only use endogenous genes. Example of a CPM function in R: calc_cpm &lt;- function (expr_mat, spikes = NULL) { norm_factor &lt;- colSums(expr_mat[-spikes, ]) return(t(t(expr_mat)/norm_factor)) * 10^6 } One potential drawback of CPM is if your sample contains genes that are both very highly expressed and differentially expressed across the cells. In this case, the total molecules in the cell may depend of whether such genes are on/off in the cell and normalizing by total molecules may hide the differential expression of those genes and/or falsely create differential expression for the remaining genes. Note RPKM, FPKM and TPM are variants on CPM which further adjust counts by the length of the respective gene/transcript. To deal with this potentiality several other measures were devised. 5.4.3.2 RLE (SF) The size factor (SF) was proposed and popularized by DESeq (Anders and Huber 2010). First the geometric mean of each gene across all cells is calculated. The size factor for each cell is the median across genes of the ratio of the expression to the gene’s geometric mean. A drawback to this method is that since it uses the geometric mean only genes with non-zero expression values across all cells can be used in its calculation, making it unadvisable for large low-depth scRNASeq experiments. edgeR &amp; scater call this method RLE for “relative log expression”. Example of a SF function in R: calc_sf &lt;- function (expr_mat, spikes = NULL) { geomeans &lt;- exp(rowMeans(log(expr_mat[-spikes, ]))) SF &lt;- function(cnts) { median((cnts/geomeans)[(is.finite(geomeans) &amp; geomeans &gt; 0)]) } norm_factor &lt;- apply(expr_mat[-spikes, ], 2, SF) return(t(t(expr_mat)/norm_factor)) } 5.4.3.3 UQ The upperquartile (UQ) was proposed by (Bullard et al. 2010). Here each column is divided by the 75% quantile of the counts for each library. Often the calculated quantile is scaled by the median across cells to keep the absolute level of expression relatively consistent. A drawback to this method is that for low-depth scRNASeq experiments the large number of undetected genes may result in the 75% quantile being zero (or close to it). This limitation can be overcome by generalizing the idea and using a higher quantile (eg. the 99% quantile is the default in scater) or by excluding zeros prior to calculating the 75% quantile. Example of a UQ function in R: calc_uq &lt;- function (expr_mat, spikes = NULL) { UQ &lt;- function(x) { quantile(x[x &gt; 0], 0.75) } uq &lt;- unlist(apply(expr_mat[-spikes, ], 2, UQ)) norm_factor &lt;- uq/median(uq) return(t(t(expr_mat)/norm_factor)) } 5.4.3.4 TMM Another method is called TMM is the weighted trimmed mean of M-values (to the reference) proposed by (Robinson and Oshlack 2010). The M-values in question are the gene-wise log2 fold changes between individual cells. One cell is used as the reference then the M-values for each other cell is calculated compared to this reference. These values are then trimmed by removing the top and bottom ~30%, and the average of the remaining values is calculated by weighting them to account for the effect of the log scale on variance. Each non-reference cell is multiplied by the calculated factor. Two potential issues with this method are insufficient non-zero genes left after trimming, and the assumption that most genes are not differentially expressed. 5.4.3.5 scran scran package implements a variant on CPM specialized for single-cell data (L. Lun, Bach, and Marioni 2016). Briefly this method deals with the problem of vary large numbers of zero values per cell by pooling cells together calculating a normalization factor (similar to CPM) for the sum of each pool. Since each cell is found in many different pools, cell-specific factors can be deconvoluted from the collection of pool-specific factors using linear algebra. 5.4.3.6 Downsampling A final way to correct for library size is to downsample the expression matrix so that each cell has approximately the same total number of molecules. The benefit of this method is that zero values will be introduced by the down sampling thus eliminating any biases due to differing numbers of detected genes. However, the major drawback is that the process is not deterministic so each time the downsampling is run the resulting expression matrix is slightly different. Thus, often analyses must be run on multiple downsamplings to ensure results are robust. Example of a downsampling function in R: Down_Sample_Matrix &lt;- function (expr_mat) { min_lib_size &lt;- min(colSums(expr_mat)) down_sample &lt;- function(x) { prob &lt;- min_lib_size/sum(x) return(unlist(lapply(x, function(y) { rbinom(1, y, prob) }))) } down_sampled_mat &lt;- apply(expr_mat, 2, down_sample) return(down_sampled_mat) } 5.4.4 Effectiveness To compare the efficiency of different normalization methods we will use visual inspection of PCA plots and calculation of cell-wise relative log expression via scater’s plotRLE() function. Namely, cells with many (few) reads have higher (lower) than median expression for most genes resulting in a positive (negative) RLE across the cell, whereas normalized cells have an RLE close to zero. Example of a RLE function in R: calc_cell_RLE &lt;- function (expr_mat, spikes = NULL) { RLE_gene &lt;- function(x) { if (median(unlist(x)) &gt; 0) { log((x + 1)/(median(unlist(x)) + 1))/log(2) } else { rep(NA, times = length(x)) } } if (!is.null(spikes)) { RLE_matrix &lt;- t(apply(expr_mat[-spikes, ], 1, RLE_gene)) } else { RLE_matrix &lt;- t(apply(expr_mat, 1, RLE_gene)) } cell_RLE &lt;- apply(RLE_matrix, 2, median, na.rm = T) return(cell_RLE) } Note The RLE, TMM, and UQ size-factor methods were developed for bulk RNA-seq data and, depending on the experimental context, may not be appropriate for single-cell RNA-seq data, as their underlying assumptions may be problematically violated. Note scater acts as a wrapper for the calcNormFactors() function from edgeR which implements several library size normalization methods making it easy to apply any of these methods to our data. Note edgeR makes extra adjustments to some of the normalization methods which may result in somewhat different results than if the original methods are followed exactly, e.g. edgeR’s and scater’s “RLE” method which is based on the “size factor” used by DESeq may give different results to the estimateSizeFactorsForMatrix() method in the DESeq/DESeq2 packages. In addition, some versions of edgeR will not calculate the normalization factors correctly unless lib.size is set at 1 for all cells. Note For CPM normalisation we use scater’s calculateCPM() function. For RLE, UQ and TMM we use scater’s normaliseExprs() function. For scran we use scran package to calculate size factors (it also operates on SingleCellExperiment class) and scater’s normalize() to normalise the data. All these normalization functions save the results to the logcounts slot of the SCE object. For downsampling we use our own functions shown above. 5.5 Normalization practice We will continue to work with the tung data that was used in the previous section. 5.5.1 Raw plotPCA( umi.qc[endog_genes, ], colour_by = &quot;batch&quot;, size_by = &quot;total_features_by_counts&quot;, shape_by = &quot;individual&quot;, run_args=list(exprs_values=&quot;logcounts_raw&quot;) ) Figure 5.16: PCA plot of the tung data Relative log expression (RLE) plots are a powerful tool for visualizing unwanted variation in high-dimensional data. These plots were originally devised for gene expression data from microarrays but can also be used on single-cell expression data. RLE plots are particularly useful for assessing whether a procedure aimed at removing unwanted variation (e.g., scaling normalization) has been successful. Here, we use the plotRLE() function in scater: plotRLE( umi.qc[endog_genes, ], exprs_values = &quot;logcounts_raw&quot;, exprs_logged = TRUE, colour_by = &quot;batch&quot; ) + ggtitle(&quot;Raw or no normalization&quot;) + ylim(-4,4) 5.5.2 CPM logcounts(umi.qc) &lt;- log2(calculateCPM(umi.qc, exprs_values = &quot;counts&quot;, use_size_factors = FALSE) + 1) plotPCA( umi.qc[endog_genes, ], colour_by = &quot;batch&quot;, size_by = &quot;total_features_by_counts&quot;, shape_by = &quot;individual&quot;, run_args = list(exprs_values=&quot;logcounts&quot;) ) Figure 5.17: PCA plot of the tung data after CPM normalisation plotRLE( umi.qc[endog_genes, ], exprs_values = &quot;logcounts&quot;, exprs_logged = TRUE, colour_by = &quot;batch&quot;) + ggtitle(&quot;After CPM normalization&quot;) + ylim(-4,4) Figure 5.18: Cell-wise RLE of the tung data 5.5.3 scran A normalization method developed specifically for scRNA-seq data is in the scran R/Bioconductor package from Lun et al. (2018). Because there are so many zeros in scRNA-seq data, calculating scaling factors for each cell is not straightforward. The idea is to sum expression values across pools of cells, and the summed values are used for normalization. Pool-based size factors are then deconvolved to yield cell-based factors. This approach (and others that have been developed specifically for scRNA-seq) has been shown to be more accurate than methods developed for bulk RNA-seq. These normalization approaches improve the results for downstream analyses. library(scran) qclust &lt;- quickCluster(umi.qc, min.size = 30) umi.qc &lt;- computeSumFactors(umi.qc, sizes = 15, clusters = qclust) umi.qc &lt;- normalize(umi.qc) plotPCA( umi.qc[endog_genes, ], colour_by = &quot;batch&quot;, size_by = &quot;total_features_by_counts&quot;, shape_by = &quot;individual&quot; ) Figure 5.19: PCA plot of the tung data after LSF normalisation plotRLE( umi.qc[endog_genes, ], exprs_values = &quot;logcounts&quot;, exprs_logged = TRUE, colour_by = &quot;batch&quot; ) + ggtitle(&quot;After scran normalization&quot;) + ylim(-4,4) Figure 5.20: Cell-wise RLE of the tung data Note: scran sometimes calculates negative or zero size factors. These will completely distort the normalized expression matrix. We can check the size factors scran has computed like so: summary(sizeFactors(umi.qc)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.4671 0.7787 0.9483 1.0000 1.1525 3.1910 For this dataset all the size factors are reasonable so we are done. If you find scran has calculated negative size factors try increasing the cluster and pool sizes until they are all positive. 5.6 Dealing with confounders 5.6.1 Introduction In the previous section we normalized for library size, effectively removing it as a confounder. Now we will consider removing other less well defined confounders from our data. Technical confounders (aka batch effects) can arise from difference in reagents, isolation methods, the lab/experimenter who performed the experiment, even which day/time the experiment was performed. Accounting for technical confounders, and batch effects particularly, is a large topic that also involves principles of experimental design. Here we address approaches that can be taken to account for confounders when the experimental design is appropriate. Fundamentally, accounting for technical confounders involves identifying and, ideally, removing sources of variation in the expression data that are not related to (i.e. are confounding) the biological signal of interest. Various approaches exist, some of which use spike-in or housekeeping genes, and some of which use endogenous genes. 5.6.1.1 Advantages and disadvantages of using spike-ins to remove confounders The use of spike-ins as control genes is appealing, since the same amount of ERCC (or other) spike-in was added to each cell in our experiment. In principle, all the variablity we observe for these genes is due to technical noise; whereas endogenous genes are affected by both technical noise and biological variability. Technical noise can be removed by fitting a model to the spike-ins and “substracting” this from the endogenous genes. There are several methods available based on this premise (eg. BASiCS, scLVM, RUVg); each using different noise models and different fitting procedures. Alternatively, one can identify genes which exhibit significant variation beyond technical noise (eg. Distance to median, Highly variable genes). However, there are issues with the use of spike-ins for normalisation (particularly ERCCs, derived from bacterial sequences), including that their variability can, for various reasons, actually be higher than that of endogenous genes. Given the issues with using spike-ins, better results can often be obtained by using endogenous genes instead. Where we have a large number of endogenous genes that, on average, do not vary systematically between cells and where we expect technical effects to affect a large number of genes (a very common and reasonable assumption), then such methods (for example, the RUVs method) can perform well. We explore both general approaches below. library(kBET) library(sva) # Combat library(edgeR) Factors contributing to technical noise frequently appear as “batch effects” where cells processed on different days or by different technicians systematically vary from one another. Removing technical noise and correcting for batch effects can frequently be performed using the same tool or slight variants on it. In the following subsections, we will consider ComBat, the use of generalized linear models (GLMs) and mnnCorrect. 5.6.2 Combat If you have an experiment with a balanced design, Combat can be used to eliminate batch effects while preserving biological effects by specifying the biological effects using the mod parameter. However the Tung data contains multiple experimental replicates rather than a balanced design so using mod1 to preserve biological variability will result in an error. combat_data &lt;- logcounts(umi.qc) mod_data &lt;- as.data.frame(t(combat_data)) # Basic batch removal mod0 = model.matrix(~ 1, data = mod_data) # Preserve biological variability mod1 = model.matrix(~ umi.qc$individual, data = mod_data) # adjust for total genes detected mod2 = model.matrix(~ umi.qc$total_features_by_counts, data = mod_data) assay(umi.qc, &quot;combat&quot;) &lt;- ComBat( dat = t(mod_data), batch = factor(umi.qc$batch), mod = mod0, par.prior = TRUE, prior.plots = FALSE ) ## Standardizing Data across genes We could also perform ComBat correction accounting for other covariates such as total_features_by_counts as a co-variate. Store the corrected matrix in the combat_tf slot. ## Standardizing Data across genes 5.6.3 mnnCorrect mnnCorrect (Haghverdi et al. 2017) assumes that each batch shares at least one biological condition with each other batch. Thus it works well for a variety of balanced experimental designs. However, the Tung data contains multiple replicates for each invidividual rather than balanced batches, thus we will normalized each individual separately. Note that this will remove batch effects between batches within the same individual but not the batch effects between batches in different individuals, due to the confounded experimental design. Thus we will merge a replicate from each individual to form three batches. do_mnn &lt;- function(data.qc) { batch1 &lt;- logcounts(data.qc[, data.qc$replicate == &quot;r1&quot;]) batch2 &lt;- logcounts(data.qc[, data.qc$replicate == &quot;r2&quot;]) batch3 &lt;- logcounts(data.qc[, data.qc$replicate == &quot;r3&quot;]) if (ncol(batch2) &gt; 0) { x = mnnCorrect( batch1, batch2, batch3, k = 20, sigma = 0.1, cos.norm.in = TRUE, svd.dim = 2 ) res1 &lt;- x$corrected[[1]] res2 &lt;- x$corrected[[2]] res3 &lt;- x$corrected[[3]] dimnames(res1) &lt;- dimnames(batch1) dimnames(res2) &lt;- dimnames(batch2) dimnames(res3) &lt;- dimnames(batch3) return(cbind(res1, res2, res3)) } else { x = mnnCorrect( batch1, batch3, k = 20, sigma = 0.1, cos.norm.in = TRUE, svd.dim = 2 ) res1 &lt;- x$corrected[[1]] res3 &lt;- x$corrected[[2]] dimnames(res1) &lt;- dimnames(batch1) dimnames(res3) &lt;- dimnames(batch3) return(cbind(res1, res3)) } } indi1 &lt;- do_mnn(umi.qc[, umi.qc$individual == &quot;NA19098&quot;]) indi2 &lt;- do_mnn(umi.qc[, umi.qc$individual == &quot;NA19101&quot;]) indi3 &lt;- do_mnn(umi.qc[, umi.qc$individual == &quot;NA19239&quot;]) assay(umi.qc, &quot;mnn&quot;) &lt;- cbind(indi1, indi2, indi3) # For a balanced design: #assay(umi.qc, &quot;mnn&quot;) &lt;- mnnCorrect( # list(B1 = logcounts(batch1), B2 = logcounts(batch2), B3 = logcounts(batch3)), # k = 20, # sigma = 0.1, # cos.norm = TRUE, # svd.dim = 2 #) 5.6.4 GLM A general linear model is a simpler version of Combat. It can correct for batches while preserving biological effects if you have a balanced design. In a confounded/replicate design biological effects will not be fit/preserved. Similar to mnnCorrect we could remove batch effects from each individual separately in order to preserve biological (and technical) variance between individuals. For demonstation purposes we will naively correct all cofounded batch effects: glm_fun &lt;- function(g, batch, indi) { model &lt;- glm(g ~ batch + indi) model$coef[1] &lt;- 0 return(model$coef) } effects &lt;- apply( logcounts(umi.qc), 1, glm_fun, batch = umi.qc$batch, indi = umi.qc$individual ) corrected &lt;- logcounts(umi.qc) - t(effects[as.numeric(factor(umi.qc$batch)), ]) assay(umi.qc, &quot;glm&quot;) &lt;- corrected In contrast, we can also perform GLM correction for each individual separately and store the final corrected matrix in the glm_indi slot. Let’s save the saveRDS(umi.qc, &quot;data/tung/umi_qc.RDS&quot;) 5.6.5 How to evaluate and compare confounder removal strategies A key question when considering the different methods for removing confounders is how to quantitatively determine which one is the most effective. The main reason why comparisons are challenging is because it is often difficult to know what corresponds to technical counfounders and what is interesting biological variability. Here, we consider three different metrics which are all reasonable based on our knowledge of the experimental design. Depending on the biological question that you wish to address, it is important to choose a metric that allows you to evaluate the confounders that are likely to be the biggest concern for the given situation. 5.6.5.1 Effectiveness 1 We evaluate the effectiveness of the normalization by inspecting the PCA plot where colour corresponds the technical replicates and shape corresponds to different biological samples (individuals). Separation of biological samples andinterspersed batches indicates that technical variation has beenremoved. We always use log2-cpm normalized data to match the assumptions of PCA. for(n in assayNames(umi.qc)) { print( plotPCA( umi.qc[endog_genes, ], colour_by = &quot;batch&quot;, size_by = &quot;total_features_by_counts&quot;, shape_by = &quot;individual&quot;, run_args = list(exprs_values = n) ) + ggtitle(n) ) } 5.6.5.2 Effectiveness 2 We can repeat the analysis using the plotExplanatoryVariables() to check whether batch effects have been removed. for(n in assayNames(umi.qc)) { print( plotExplanatoryVariables( umi.qc[endog_genes, ], variables = c( &quot;total_features_by_counts&quot;, &quot;total_counts&quot;, &quot;batch&quot;, &quot;individual&quot;, &quot;pct_counts_ERCC&quot;, &quot;pct_counts_MT&quot; ), exprs_values=n) + ggtitle(n) ) } Figure 5.21: Explanatory variables (mnn) Figure 5.21: Explanatory variables (mnn) Figure 5.21: Explanatory variables (mnn) Figure 5.21: Explanatory variables (mnn) Figure 5.21: Explanatory variables (mnn) Figure 5.21: Explanatory variables (mnn) Figure 5.21: Explanatory variables (mnn) Figure 5.21: Explanatory variables (mnn) 5.6.5.3 Effectiveness 3 Another method to check the efficacy of batch-effect correction is to consider the intermingling of points from different batches in local subsamples of the data. If there are no batch-effects then proportion of cells from each batch in any local region should be equal to the global proportion of cells in each batch. kBET (Buttner et al. 2017) takes kNN networks around random cells and tests the number of cells from each batch against a binomial distribution. The rejection rate of these tests indicates the severity of batch-effects still present in the data (high rejection rate = strong batch effects). kBET assumes each batch contains the same complement of biological groups, thus it can only be applied to the entire dataset if a perfectly balanced design has been used. However, kBET can also be applied to replicate-data if it is applied to each biological group separately. In the case of the Tung data, we will apply kBET to each individual independently to check for residual batch effects. However, this method will not identify residual batch-effects which are confounded with biological conditions. In addition, kBET does not determine if biological signal has been preserved. compare_kBET_results &lt;- function(sce){ indiv &lt;- unique(sce$individual) norms &lt;- assayNames(sce) # Get all normalizations results &lt;- list() for (i in indiv){ for (j in norms){ tmp &lt;- kBET( df = t(assay(sce[,sce$individual== i], j)), batch = sce$batch[sce$individual==i], heuristic = TRUE, verbose = FALSE, addTest = FALSE, plot = FALSE) results[[i]][[j]] &lt;- tmp$summary$kBET.observed[1] } } return(as.data.frame(results)) } eff_debatching &lt;- compare_kBET_results(umi.qc) library(reshape2) library(RColorBrewer) # Plot results dod &lt;- melt(as.matrix(eff_debatching), value.name = &quot;kBET&quot;) colnames(dod)[1:2] &lt;- c(&quot;Normalisation&quot;, &quot;Individual&quot;) colorset &lt;- c(&#39;gray&#39;, brewer.pal(n = 9, &quot;RdYlBu&quot;)) ggplot(dod, aes(Normalisation, Individual, fill=kBET)) + geom_tile() + scale_fill_gradient2( na.value = &quot;gray&quot;, low = colorset[2], mid=colorset[6], high = colorset[10], midpoint = 0.5, limit = c(0,1)) + scale_x_discrete(expand = c(0, 0)) + scale_y_discrete(expand = c(0, 0)) + theme( axis.text.x = element_text( angle = 45, vjust = 1, size = 12, hjust = 1 ) ) + ggtitle(&quot;Effect of batch regression methods per individual&quot;) ## Loading required package: SummarizedExperiment ## Loading required package: GenomicRanges ## Loading required package: stats4 ## Loading required package: BiocGenerics ## Loading required package: parallel ## ## Attaching package: &#39;BiocGenerics&#39; ## The following objects are masked from &#39;package:parallel&#39;: ## ## clusterApply, clusterApplyLB, clusterCall, clusterEvalQ, ## clusterExport, clusterMap, parApply, parCapply, parLapply, ## parLapplyLB, parRapply, parSapply, parSapplyLB ## The following objects are masked from &#39;package:stats&#39;: ## ## IQR, mad, sd, var, xtabs ## The following objects are masked from &#39;package:base&#39;: ## ## anyDuplicated, append, as.data.frame, basename, cbind, ## colMeans, colnames, colSums, dirname, do.call, duplicated, ## eval, evalq, Filter, Find, get, grep, grepl, intersect, ## is.unsorted, lapply, lengths, Map, mapply, match, mget, order, ## paste, pmax, pmax.int, pmin, pmin.int, Position, rank, rbind, ## Reduce, rowMeans, rownames, rowSums, sapply, setdiff, sort, ## table, tapply, union, unique, unsplit, which, which.max, ## which.min ## Loading required package: S4Vectors ## ## Attaching package: &#39;S4Vectors&#39; ## The following object is masked from &#39;package:base&#39;: ## ## expand.grid ## Loading required package: IRanges ## Loading required package: GenomeInfoDb ## Loading required package: Biobase ## Welcome to Bioconductor ## ## Vignettes contain introductory material; view with ## &#39;browseVignettes()&#39;. To cite Bioconductor, see ## &#39;citation(&quot;Biobase&quot;)&#39;, and for packages &#39;citation(&quot;pkgname&quot;)&#39;. ## Loading required package: DelayedArray ## Loading required package: matrixStats ## ## Attaching package: &#39;matrixStats&#39; ## The following objects are masked from &#39;package:Biobase&#39;: ## ## anyMissing, rowMedians ## Loading required package: BiocParallel ## ## Attaching package: &#39;DelayedArray&#39; ## The following objects are masked from &#39;package:matrixStats&#39;: ## ## colMaxs, colMins, colRanges, rowMaxs, rowMins, rowRanges ## The following objects are masked from &#39;package:base&#39;: ## ## aperm, apply ## Loading required package: ggplot2 ## ## Attaching package: &#39;scater&#39; ## The following object is masked from &#39;package:S4Vectors&#39;: ## ## rename ## The following object is masked from &#39;package:stats&#39;: ## ## filter References "],
["biological-analysis.html", "6 Biological Analysis 6.1 Clustering 6.2 Clustering example 6.3 Feature Selection 6.4 Differential Expression (DE) analysis 6.5 DE in tung dataset", " 6 Biological Analysis 6.1 Clustering Once we have normalized the data and removed confounders we can carry out analyses that are relevant to the biological questions at hand. The exact nature of the analysis depends on the dataset. Nevertheless, there are a few aspects that are useful in a wide range of contexts and we will be discussing some of them in the next few chapters. We will start with the clustering of scRNA-seq data. 6.1.1 Introduction One of the most promising applications of scRNA-seq is de novo discovery and annotation of cell-types based on transcription profiles. Computationally, this is a hard problem as it amounts to unsupervised clustering. That is, we need to identify groups of cells based on the similarities of the transcriptomes without any prior knowledge of the labels. Moreover, in most situations we do not even know the number of clusters a priori. The problem is made even more challenging due to the high level of noise (both technical and biological) and the large number of dimensions (i.e. genes). 6.1.2 Dimensionality reduction When working with large datasets, it can often be beneficial to apply some sort of dimensionality reduction method. By projecting the data onto a lower-dimensional sub-space, one is often able to significantly reduce the amount of noise. An additional benefit is that it is typically much easier to visualize the data in a 2 or 3-dimensional subspace. We have already discussed PCA and t-SNE. 6.1.3 Clustering methods Unsupervised clustering is useful in many different applications and it has been widely studied in machine learning. Some of the most popular approaches are hierarchical clustering, k-means clustering and graph-based clustering. 6.1.3.1 Hierarchical clustering In hierarchical clustering, one can use either a bottom-up or a top-down approach. In the former case, each cell is initially assigned to its own cluster and pairs of clusters are subsequently merged to create a hieararchy: knitr::include_graphics(&quot;figures/hierarchical_clustering2.png&quot;) Figure 6.1: The hierarchical clustering dendrogram With a top-down strategy, one instead starts with all observations in one cluster and then recursively split each cluster to form a hierarchy. One of the advantages of this strategy is that the method is deterministic. 6.1.3.2 k-means In k-means clustering, the goal is to partition N cells into k different clusters. In an iterative manner, cluster centers are assigned and each cell is assigned to its nearest cluster: knitr::include_graphics(&quot;figures/k-means.png&quot;) Figure 6.2: Schematic representation of the k-means clustering Most methods for scRNA-seq analysis includes a k-means step at some point. 6.1.3.3 Graph-based methods Over the last two decades there has been a lot of interest in analyzing networks in various domains. One goal is to identify groups or modules of nodes in a network. knitr::include_graphics(&quot;figures/graph_network.jpg&quot;) Figure 6.3: Schematic representation of the graph network Some of these methods can be applied to scRNA-seq data by building a graph where each node represents a cell. Note that constructing the graph and assigning weights to the edges is not trivial. One advantage of graph-based methods is that some of them are very efficient and can be applied to networks containing millions of nodes. 6.1.4 Challenges in clustering What is the number of clusters k? What is a cell type? Scalability: in the last few years the number of cells in scRNA-seq experiments has grown by several orders of magnitude from ~\\(10^2\\) to ~\\(10^6\\) Tools are not user-friendly 6.1.5 Software packages for scRNA-seq data 6.1.5.1 clusterExperiment R/Bioconductor package uses SingleCellExperiment object Functionality for running and comparing many different clusterings of single-cell sequencing data or other large mRNA expression data sets Based on the RSEC (Resampling-based Sequential Ensemble Clustering) algorithm for finding a single robust clustering based on the many clusterings that the user might create by perturbing various parameters of a clustering algorithm 6.1.5.2 SC3 knitr::include_graphics(&quot;figures/sc3.png&quot;) Figure 6.4: SC3 pipeline R/Bioconductor package based on PCA and spectral dimensionality reductions (Kiselev et al. 2017) Utilises k-means Additionally performs the consensus clustering 6.1.5.3 tSNE + k-means Based on tSNE maps Utilises k-means 6.1.5.4 SINCERA SINCERA (Guo et al. 2015) is based on hierarchical clustering Data is converted to z-scores before clustering Identify k by finding the first singleton cluster in the hierarchy 6.1.5.5 pcaReduce pcaReduce (žurauskienė and Yau 2016) combines PCA, k-means and “iterative” hierarchical clustering. Starting from a large number of clusters pcaReduce iteratively merges similar clusters; after each merging event it removes the principle component explaning the least variance in the data. 6.1.5.6 SNN-Cliq SNN-Cliq (Xu and Su 2015) is a graph-based method. First the method identifies the k-nearest-neighbours of each cell according to the distance measure. This is used to calculate the number of Shared Nearest Neighbours (SNN) between each pair of cells. A graph is built by placing an edge between two cells If they have at least one SNN. Clusters are defined as groups of cells with many edges between them using a “clique” method. SNN-Cliq requires several parameters to be defined manually. 6.1.5.7 Seurat clustering Seurat clustering is based on a community detection approach similar to SNN-Cliq and to one previously proposed for analyzing CyTOF data (Levine et al. 2015). Since Seurat has become more like an all-in-one tool for scRNA-seq data analysis we dedicate a separate chapter to discuss it in more details (chapter ??). 6.1.6 Comparing clustering To compare two sets of clustering labels we can use adjusted Rand index. The index is a measure of the similarity between two data clusterings. Values of the adjusted Rand index lie in \\([0;1]\\) interval, where \\(1\\) means that two clusterings are identical and \\(0\\) means the level of similarity expected by chance. 6.2 Clustering example library(SC3) To illustrate clustering of scRNA-seq data, we consider the Deng dataset of cells from developing mouse embryo (Deng et al. 2014). We have preprocessed the dataset and created a SingleCellExperiment object in advance. We have also annotated the cells with the cell types identified in the original publication (it is the cell_type2 column in the colData slot). 6.2.1 Deng dataset Let’s load the data and look at it: deng &lt;- readRDS(&quot;data/deng/deng-reads.rds&quot;) deng ## class: SingleCellExperiment ## dim: 22431 268 ## metadata(0): ## assays(2): counts logcounts ## rownames(22431): Hvcn1 Gbp7 ... Sox5 Alg11 ## rowData names(10): feature_symbol is_feature_control ... ## total_counts log10_total_counts ## colnames(268): 16cell 16cell.1 ... zy.2 zy.3 ## colData names(30): cell_type2 cell_type1 ... pct_counts_ERCC ## is_cell_control ## reducedDimNames(0): ## spikeNames(1): ERCC Let’s look at the cell type annotation: table(colData(deng)$cell_type2) ## ## 16cell 4cell 8cell early2cell earlyblast late2cell ## 50 14 37 8 43 10 ## lateblast mid2cell midblast zy ## 30 12 60 4 A simple PCA analysis already separates some strong cell types and provides some insights in the data structure: plotPCA(deng, colour_by = &quot;cell_type2&quot;) As you can see, the early cell types separate quite well, but the three blastocyst timepoints are more difficult to distinguish. 6.2.2 SC3 Let’s run SC3 clustering on the Deng data. The advantage of the SC3 is that it can directly ingest a SingleCellExperiment object. Now let’s image we do not know the number of clusters k (cell types). SC3 can estimate a number of clusters for you: deng &lt;- sc3_estimate_k(deng) ## Estimating k... metadata(deng)$sc3$k_estimation ## [1] 6 Interestingly, the number of cell types predicted by SC3 is smaller than in the original data annotation. However, early, mid and late stages of different cell types together, we will have exactly 6 cell types. We store the merged cell types in cell_type1 column of the colData slot: plotPCA(deng, colour_by = &quot;cell_type1&quot;) Now we are ready to run SC3 (we also ask it to calculate biological properties of the clusters) deng &lt;- sc3(deng, ks = 10, biology = TRUE) SC3 result consists of several different outputs (please look in (Kiselev et al. 2017) and SC3 vignette for more details). Here we show some of them: First up we plot a “consensus matrix”. The consensus matrix is a NxN matrix, where N is the number of cells. It represents similarity between the cells based on the averaging of clustering results from all combinations of clustering parameters. Similarity 0 (blue) means that the two cells are always assigned to different clusters. In contrast, similarity 1 (red) means that the two cells are always assigned to the same cluster. The consensus matrix is clustered by hierarchical clustering and has a diagonal-block structure. Intuitively, the perfect clustering is achieved when all diagonal blocks are completely red and all off-diagonal elements are completely blue. We can plot the consensus matrix as a heatmap using the sc3_plot_consensus() in the SC3 package. sc3_plot_consensus(deng, k = 10, show_pdata = &quot;cell_type2&quot;) Next, we can create a heatmap of the gene expression matrix using the sc3_plot_expression() function in SC3. The expression heatmap represents the original input expression matrix (cells in columns and genes in rows) after applying a gene filter. Genes are clustered by kmeans with k = 100 (dendrogram on the left) and the heatmap represents the expression levels of the gene cluster centers after log2-scaling. We also ask for k = 10 clusters of cells. sc3_plot_expression(deng, k = 10, show_pdata = &quot;cell_type2&quot;) We can also create PCA plot with clusters identified from SC3: plotPCA(deng, colour_by = &quot;sc3_10_clusters&quot;) We can compare the results of SC3 clustering with the original publication cell type labels using the Adjusted Rand Index. The Rand index (named after William M. Rand) is a measure of the similarity between two data clusterings. The adjusted Rand index is the Rand index that is adjusted for the chance grouping of elements. Such a correction for chance establishes a baseline by using the expected similarity of all pair-wise comparisons between clusterings specified by a random model. The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical. Here, we use the function adjustedRandIndex(), which is part of the mclust package: mclust::adjustedRandIndex(colData(deng)$cell_type2, colData(deng)$sc3_10_clusters) ## [1] 0.6638246 Note SC3 can also be run in an interactive Shiny session: sc3_interactive(deng) This command will open SC3 in a web browser. Before we leave this section, the package vignette points out that because of direct calculations of distances, the SC3 functions will become very slow when the number of cells is \\(&gt;5000\\). 6.2.3 tSNE + kmeans tSNE plots that we saw before when used the scater package are made by using the Rtsne and ggplot2 packages. Here we will do the same: deng &lt;- runTSNE(deng) plotTSNE(deng) Figure 6.5: tSNE map of the patient data Note that all points on the plot above are black. This is different from what we saw before, when the cells were coloured based on the annotation. Here we do not have any annotation and all cells come from the same batch, therefore all dots are black. Now we are going to apply k-means clustering algorithm to the cloud of points on the tSNE map. How many groups do you see in the cloud? We will start with \\(k=8\\): colData(deng)$tSNE_kmeans &lt;- as.character(kmeans(reducedDim(deng, &quot;TSNE&quot;), centers = 8)$clust) deng &lt;- runTSNE(deng) plotTSNE(deng, colour_by = &quot;tSNE_kmeans&quot;) ## Warning: &#39;add_ticks&#39; is deprecated. ## Use &#39;+ geom_rug(...)&#39; instead. Figure 6.6: tSNE map of the patient data with 8 colored clusters, identified by the k-means clustering algorithm As you may have noticed, tSNE+kmeans is stochastic and give different results every time it’s are run. To get a better overview of the solutions, we need to run the methods multiple times. SC3 is also stochastic, but thanks to the consensus step, it is more robust and less likely to produce different outcomes. 6.3 Feature Selection scRNA-seq is capable of measuring the expression of many thousands of genes in every cell. However, in most situations only a portion of those will show a response to the biological condition of interest, e.g. differences in cell-type, drivers of differentiation, respond to an environmental stimulus. Most genes detected in a scRNA-seq experiment will only be detected at different levels due to technical noise. One consequence of this is that technical noise and batch effects can obscure the biological signal of interest. Thus, it is often advantageous to perform feature selection to remove those genes which only exhibit technical noise from downstream analysis. Not only does this generally increase the signal:noise ratio in the data; it also reduces the computational complexity of analyses, by reducing the total amount of data to be processed. For scRNA-seq data, we typically focus on unsupervised methods of feature selection, which don’t require any a priori information, such as cell-type labels or biological group, since they are not available, or may be unreliable, for many experiments. In contrast, differential expression can be considered a form of supervised feature selection since it uses the known biological label of each sample to identify features (i.e. genes) which are expressed at different levels across groups. In this section, we will briefly cover the concepts of feature selection, but due to a limit in time, I refer you to the main coures page for more details on feature selection: http://hemberg-lab.github.io/scRNA.seq.course/biological-analysis.html#feature-selection 6.3.1 Identifying Genes vs a Null Model There are two main approaches to unsupervised feature selection. The first is to identify genes which behave differently from a null model describing just the technical noise expected in the dataset. If the dataset contains spike-in RNAs they can be used to directly model technical noise. However, measurements of spike-ins may not experience the same technical noise as endogenous transcripts (Svensson et al., 2017). In addition, scRNASeq experiments often contain only a small number of spike-ins which reduces our confidence in fitted model parameters. 6.3.1.1 Highly Variable Genes The first method proposed to identify features in scRNASeq datasets was to identify highly variable genes (HVG). HVG assumes that if genes have large differences in expression across cells some of those differences are due to biological difference between the cells rather than technical noise. However, because of the nature of count data, there is a positive relationship between the mean expression of a gene and the variance in the read counts across cells. This relationship must be corrected for to properly identify HVGs. For example, you can use the BiocGenerics::rowMeans() and MatrixStats::rowVars() functions to plot the relationship between mean expression and variance for all genes in this dataset. Also, you might try using log=&quot;xy&quot; to plot on a log-scale). Alternatively, you can use the trendVar() and decomposeVar() functions in the scran R/Bioconductor package to identify highly variable genes. For this section, we go back to the tung (or umi.qc) data that has been normalized and batch corrected. For purposes of the tutorial, we will select the glm_indi normalized data. umi.qc &lt;- readRDS(&quot;data/tung/umi_qc.RDS&quot;) var.fit &lt;- trendVar(umi.qc, method = &quot;loess&quot;, assay.type = &quot;glm_indi&quot;) var.out &lt;- decomposeVar(umi.qc, var.fit) head(var.out) ## DataFrame with 6 rows and 6 columns ## mean total bio ## &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; ## ENSG00000237683 0.24662736578077 0.243858690202493 0.0356801534015803 ## ENSG00000187634 0.0358302498678336 0.041042573799668 0.00447500876958429 ## ENSG00000188976 1.71485831826354 0.745273783518252 0.114898539528563 ## ENSG00000187961 0.218414586619208 0.203530230554452 0.0193960410083338 ## ENSG00000187608 1.3692218193445 0.801170759584193 0.142065201109407 ## ENSG00000188157 2.02030076886988 0.545656338513709 -0.0424745356608807 ## tech p.value ## &lt;numeric&gt; &lt;numeric&gt; ## ENSG00000237683 0.208178536800913 0.00153459146498894 ## ENSG00000187634 0.0365675650300837 0.0157736562230252 ## ENSG00000188976 0.630375243989689 0.00085174073496591 ## ENSG00000187961 0.184134189546118 0.0312433209044445 ## ENSG00000187608 0.659105558474786 0.000120336074819414 ## ENSG00000188157 0.588130874174589 0.906968345769253 ## FDR ## &lt;numeric&gt; ## ENSG00000237683 0.0190971382309735 ## ENSG00000187634 0.106015932367908 ## ENSG00000188976 0.0124082937456012 ## ENSG00000187961 0.167734905189692 ## ENSG00000187608 0.00271289057563897 ## ENSG00000188157 1 Next, we can plot the mean-variance relationship. The red points are the control genes (ERCC and MT genes) that are used to estimate the technical variation. The endogenous genes are plotted in black. plot(var.out$mean, var.out$total, pch=16, cex=0.6, xlab=&quot;Mean log-expression&quot;, ylab=&quot;Variance of log-expression&quot;) points(var.fit$mean, var.fit$var, col=&quot;red&quot;, pch=16) o &lt;- order(var.out$mean) lines(var.out$mean[o], var.out$tech[o], col=&quot;red&quot;, lwd=2) Then, the highly variable genes (HVG) are identified as genes with large positive biological components. hvg.out &lt;- var.out[which(var.out$FDR &lt;= 0.05),] hvg.out &lt;- hvg.out[order(hvg.out$bio, decreasing=TRUE),] head(hvg.out) ## DataFrame with 6 rows and 6 columns ## mean total bio ## &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; ## ENSG00000106153 2.58010587365124 4.50251391535177 3.99819997557498 ## ENSG00000125148 1.10385481967694 2.56632990001896 1.92729406297511 ## ENSG00000198865 2.43296165351555 2.36687881721125 1.84095133880717 ## ENSG00000110713 3.07884927706287 2.20353806082695 1.76355959764117 ## ENSG00000022556 1.87087512946706 2.20770425606636 1.60139777768658 ## ENSG00000177105 1.45436561173197 2.03125271629969 1.36969341922523 ## tech p.value ## &lt;numeric&gt; &lt;numeric&gt; ## ENSG00000106153 0.504313939776785 0 ## ENSG00000125148 0.639035837043843 1.94361011485433e-234 ## ENSG00000198865 0.525927478404086 2.74586525446619e-287 ## ENSG00000110713 0.43997846318578 0 ## ENSG00000022556 0.606306478379779 5.88519208672335e-195 ## ENSG00000177105 0.661559297074457 7.9450415594479e-138 ## FDR ## &lt;numeric&gt; ## ENSG00000106153 0 ## ENSG00000125148 5.44210832159212e-231 ## ENSG00000198865 1.28140378541756e-283 ## ENSG00000110713 0 ## ENSG00000022556 1.17703841734467e-191 ## ENSG00000177105 9.26921515268922e-135 These are most highly variable genes after normalization and batch correction. nrow(hvg.out) ## [1] 1561 We can check the distribution of expression values for the top 10 HVGs to ensure that they are not being driven by outliers. Here are the violin plots of normalized log-expression values for the top 10 HVGs in the brain dataset. plotExpression(umi.qc, rownames(hvg.out)[1:10], jitter=&quot;jitter&quot;, exprs_values = &quot;glm_indi&quot;) Another approach was proposed by Brennecke et al.. To use the Brennecke method, we first normalize for library size then calculate the mean and the square coefficient of variation (variation divided by the squared mean expression). A quadratic curve is fit to the relationship between these two variables for the ERCC spike-in, and then a chi-square test is used to find genes significantly above the curve. 6.3.1.2 High Dropout Genes An alternative to finding HVGs is to identify genes with unexpectedly high numbers of zeros. The frequency of zeros, known as the “dropout rate”, is very closely related to expression level in scRNA-seq data. Zeros are the dominant feature of scRNA-seq data, typically accounting for over half of the entries in the final expression matrix. These zeros predominantly result from the failure of mRNAs failing to be reversed transcribed (Andrews and Hemberg, 2016). 6.3.2 Correlated Expression A completely different approach to feature selection is to use gene-gene correlations. This method is based on the idea that multiple genes will be differentially expressed between different cell-types or cell-states. Genes which are expressed in the same cell-population will be positively correlated with each other where as genes expressed in different cell-populations will be negatively correated with each other. Thus important genes can be identified by the magnitude of their correlation with other genes. The limitation of this method is that it assumes technical noise is random and independent for each cell, thus shouldn’t produce gene-gene correlations, but this assumption is violated by batch effects which are generally systematic between different experimental batches and will produce gene-gene correlations. As a result it is more appropriate to take the top few thousand genes as ranked by gene-gene correlation than consider the significance of the correlations. Lastly, another common method for feature selection in scRNA-seq data is to use PCA loadings. Genes with high PCA loadings are likely to be highly variable and correlated with many other variable genes, thus may be relevant to the underlying biology. However, as with gene-gene correlations PCA loadings tend to be susceptible to detecting systematic variation due to batch effects; thus it is recommended to plot the PCA results to determine those components corresponding to the biological variation rather than batch effects. 6.4 Differential Expression (DE) analysis 6.4.1 Bulk RNA-seq One of the most common types of analyses when working with bulk RNA-seq data is to identify differentially expressed genes. By comparing the genes that change between two conditions, e.g. mutant and wild-type or stimulated and unstimulated, it is possible to characterize the molecular mechanisms underlying the change. Several different methods, e.g. DESeq2 and edgeR, have been developed for bulk RNA-seq. Moreover, there are also extensive datasets available where the RNA-seq data has been validated using RT-qPCR. These data can be used to benchmark DE finding algorithms and the available evidence suggests that the algorithms are performing quite well. 6.4.2 Single-cell RNA-seq In contrast to bulk RNA-seq, in scRNA-seq we usually do not have a defined set of experimental conditions. Instead, as was shown in a previous section we can identify the cell groups by using an unsupervised clustering approach. Once the groups have been identified one can find differentially expressed genes either by comparing the differences in variance between the groups (like the Kruskal-Wallis test implemented in SC3), or by comparing gene expression between clusters in a pairwise manner. In the following section we will mainly consider tools developed for pairwise comparisons. 6.4.3 Differences in distribution Unlike bulk RNA-seq, we generally have a large number of samples (i.e. cells) for each group we are comparing in single-cell experiments. Thus we can take advantage of the whole distribution of expression values in each group to identify differences between groups rather than only comparing estimates of mean-expression as is standard for bulk RNA-seq. There are two main approaches to comparing distributions. Firstly, we can use existing statistical models/distributions and fit the same type of model to the expression in each group then test for differences in the parameters for each model, or test whether the model fits better if a particular paramter is allowed to be different according to group. For instance in section on dealing with confounders, we used edgeR to test whether allowing mean expression to be different in different batches significantly improved the fit of a negative binomial model of the data. Alternatively, we can use a non-parametric test which does not assume that expression values follow any particular distribution, e.g. the Kolmogorov-Smirnov test (KS-test). Non-parametric tests generally convert observed expression values to ranks and test whether the distribution of ranks for one group are signficantly different from the distribution of ranks for the other group. However, some non-parametric methods fail in the presence of a large number of tied values, such as the case for dropouts (zeros) in single-cell RNA-seq expression data. Moreover, if the conditions for a parametric test hold, then it will typically be more powerful than a non-parametric test. 6.4.4 Models of single-cell RNA-seq data The most common model of RNASeq data is the negative binomial model: set.seed(1) hist( rnbinom( 1000, mu = 10, size = 100), col = &quot;grey50&quot;, xlab = &quot;Read Counts&quot;, main = &quot;Negative Binomial&quot; ) Figure 6.7: Negative Binomial distribution of read counts for a single gene across 1000 cells Mean: \\(\\mu = mu\\) Variance: \\(\\sigma^2 = mu + mu^2/size\\) It is parameterized by the mean expression (mu) and the dispersion (size), which is inversely related to the variance. The negative binomial model fits bulk RNA-seq data very well and it is used for most statistical methods designed for such data. In addition, it has been show to fit the distribution of molecule counts obtained from data tagged by unique molecular identifiers (UMIs) quite well (Grun et al. 2014, Islam et al. 2011). However, a raw negative binomial model does not fit full-length transcript data as well due to the high dropout rates relative to the non-zero read counts. For this type of data a variety of zero-inflated negative binomial models have been proposed (e.g. MAST, SCDE). d &lt;- 0.5; counts &lt;- rnbinom( 1000, mu = 10, size = 100 ) counts[runif(1000) &lt; d] &lt;- 0 hist( counts, col = &quot;grey50&quot;, xlab = &quot;Read Counts&quot;, main = &quot;Zero-inflated NB&quot; ) Figure 6.8: Zero-inflated Negative Binomial distribution Mean: \\(\\mu = mu \\cdot (1 - d)\\) Variance: \\(\\sigma^2 = \\mu \\cdot (1-d) \\cdot (1 + d \\cdot \\mu + \\mu / size)\\) These models introduce a new parameter \\(d\\), for the dropout rate, to the negative binomial model. Turns out, the dropout rate of a gene is strongly correlated with the mean expression of the gene. Different zero-inflated negative binomial models use different relationships between \\(\\mu\\) and \\(d\\) and some may fit \\(\\mu\\) and \\(d\\) to the expression of each gene independently. 6.5 DE in tung dataset library(scRNA.seq.funcs) library(edgeR) library(ROCR) 6.5.1 Introduction To test different single-cell differential expression methods, we will continue using the tung dataset. For this experiment bulk RNA-seq data for each cell-line was generated in addition to single-cell data. We will use the differentially expressed genes identified using standard methods on the respective bulk data as the ground truth for evaluating the accuracy of each single-cell method. To save time we have pre-computed these for you. You can run the commands below to load these data. DE &lt;- read.table(&quot;data/tung/TPs.txt&quot;) notDE &lt;- read.table(&quot;data/tung/TNs.txt&quot;) GroundTruth &lt;- list( DE = as.character(unlist(DE)), notDE = as.character(unlist(notDE)) ) This ground truth has been produce for the comparison of individual NA19101 to NA19239. umi.qc.sub &lt;- umi.qc[, umi.qc$individual %in% c(&quot;NA19101&quot;, &quot;NA19239&quot;)] group &lt;- colData(umi.qc.sub)$individual batch &lt;- colData(umi.qc.sub)$batch norm_data &lt;- assay(umi.qc.sub, &quot;glm_indi&quot;) Now we will compare various single-cell DE methods. Note that we will only be running methods which are available as R-packages and run relatively quickly. 6.5.2 Kolmogorov-Smirnov test The types of test that are easiest to work with are non-parametric ones. The most commonly used non-parametric test is the Kolmogorov-Smirnov test (KS-test) and we can use it to compare the distributions for each gene in the two individuals. The KS-test quantifies the distance between the empirical cummulative distributions of the expression of each gene in each of the two populations. It is sensitive to changes in mean experession and changes in variability. However it assumes data is continuous and may perform poorly when data contains a large number of identical values (e.g. zeros). Another issue with the KS-test is that it can be very sensitive for large sample sizes and thus it may end up as significant even though the magnitude of the difference is very small. Figure 6.9: Illustration of the two-sample Kolmogorov–Smirnov statistic. Red and blue lines each correspond to an empirical distribution function, and the black arrow is the two-sample KS statistic. (taken from here) Now run the test: pVals &lt;- apply( norm_data, 1, function(x) { ks.test( x[group == &quot;NA19101&quot;], x[group == &quot;NA19239&quot;] )$p.value } ) # multiple testing correction pVals &lt;- p.adjust(pVals, method = &quot;fdr&quot;) This code “applies” the function to each row (specified by 1) of the expression matrix, data. In the function, we are returning just the p.value from the ks.test() output. We can now consider how many of the ground truth positive and negative DE genes are detected by the KS-test. 6.5.2.1 Evaluating Accuracy sigDE &lt;- names(pVals)[pVals &lt; 0.05] # Number of KS-DE genes length(sigDE) ## [1] 11942 # Number of KS-DE genes that are true DE genes sum(GroundTruth$DE %in% sigDE) ## [1] 989 # Number of KS-DE genes that are truly not-DE sum(GroundTruth$notDE %in% sigDE) ## [1] 8527 As you can see many more of our ground truth negative genes were identified as DE by the KS-test (false positives) than ground truth positive genes (true positives), however this may be due to the larger number of notDE genes thus we typically normalize these counts as the True positive rate (TPR), TP/(TP + FN), and False positive rate (FPR), FP/(FP+TP). tp &lt;- sum(GroundTruth$DE %in% sigDE) fp &lt;- sum(GroundTruth$notDE %in% sigDE) tn &lt;- sum(GroundTruth$notDE %in% names(pVals)[pVals &gt;= 0.05]) fn &lt;- sum(GroundTruth$DE %in% names(pVals)[pVals &gt;= 0.05]) tpr &lt;- tp/(tp + fn) fpr &lt;- fp/(fp + tn) cat(c(tpr, fpr)) ## 0.9347826 0.8212463 Now we can see the TPR is much higher than the FPR indicating the KS test is identifying DE genes. So far we’ve only evaluated the performance at a single significance threshold. Often it is informative to vary the threshold and evaluate performance across a range of values. This is then plotted as a receiver-operating-characteristic curve (ROC) and a general accuracy statistic can be calculated as the area under this curve (AUC). We will use the ROCR package to facilitate this plotting. # Only consider genes for which we know the ground truth pVals &lt;- pVals[names(pVals) %in% GroundTruth$DE | names(pVals) %in% GroundTruth$notDE] truth &lt;- rep(1, times = length(pVals)); truth[names(pVals) %in% GroundTruth$DE] = 0; pred &lt;- ROCR::prediction(pVals, truth) perf &lt;- ROCR::performance(pred, &quot;tpr&quot;, &quot;fpr&quot;) ROCR::plot(perf) Figure 6.10: ROC curve for KS-test. aucObj &lt;- ROCR::performance(pred, &quot;auc&quot;) aucObj@y.values[[1]] # AUC ## [1] 0.6717898 Finally, to facilitate the comparisons of other DE methods, let’s put this code into a function so we don’t need to repeat it: DE_Quality_AUC &lt;- function(pVals) { pVals &lt;- pVals[names(pVals) %in% GroundTruth$DE | names(pVals) %in% GroundTruth$notDE] truth &lt;- rep(1, times = length(pVals)); truth[names(pVals) %in% GroundTruth$DE] = 0; pred &lt;- ROCR::prediction(pVals, truth) perf &lt;- ROCR::performance(pred, &quot;tpr&quot;, &quot;fpr&quot;) ROCR::plot(perf) aucObj &lt;- ROCR::performance(pred, &quot;auc&quot;) return(aucObj@y.values[[1]]) } 6.5.3 Wilcox/Mann-Whitney-U Test The Wilcox-rank-sum test is another non-parametric test, but tests specifically if values in one group are greater/less than the values in the other group. Thus it is often considered a test for difference in median expression between two groups; whereas the KS-test is sensitive to any change in distribution of expression values. pVals &lt;- apply( norm_data, 1, function(x) { wilcox.test( x[group == &quot;NA19101&quot;], x[group == &quot;NA19239&quot;] )$p.value } ) # multiple testing correction pVals &lt;- p.adjust(pVals, method = &quot;fdr&quot;) DE_Quality_AUC(pVals) Figure 6.11: ROC curve for Wilcox test. ## [1] 0.6779454 6.5.4 edgeR edgeR is based on a negative binomial model of gene expression and uses a generalized linear model (GLM) framework, the enables us to include other factors such as batch to the model. dge &lt;- DGEList( counts = assay(umi.qc.sub, &quot;counts&quot;), norm.factors = rep(1, length(assay(umi.qc.sub, &quot;counts&quot;)[1,])), group = group ) group_edgeR &lt;- factor(group) design &lt;- model.matrix(~ group_edgeR) dge &lt;- estimateDisp(dge, design = design, trend.method = &quot;none&quot;) fit &lt;- glmFit(dge, design) res &lt;- glmLRT(fit) pVals &lt;- res$table[,4] names(pVals) &lt;- rownames(res$table) pVals &lt;- p.adjust(pVals, method = &quot;fdr&quot;) DE_Quality_AUC(pVals) 6.5.5 Monocle Monocle can use several different models for DE. For count data it recommends the Negative Binomial model (negbinomial.size). For normalized data it recommends log-transforming it then using a normal distribution (gaussianff). Similar to edgeR this method uses a GLM framework so in theory can account for batches, however in practice the model fails for this dataset if batches are included. 6.5.6 MAST MAST is based on a zero-inflated negative binomial model. It tests for differential expression using a hurdle model to combine tests of discrete (0 vs not zero) and continuous (non-zero values) aspects of gene expression. Again this uses a linear modelling framework to enable complex models to be considered. 6.5.7 SCDE SCDE is the first single-cell specific DE method. It fits a zero-inflated negative binomial model to expression data using Bayesian statistics. The usage below tests for differences in mean expression of individual genes across groups but recent versions include methods to test for differences in mean expression or dispersion of groups of genes, usually representing a pathway. 6.5.8 zinbwave + DESeq2 library(zinbwave) # low count filter - at least 25 samples with count of 5 or more keep &lt;- rowSums(counts(umi.qc.sub) &gt; 0) &gt; 5 table(keep) zinb &lt;- umi.qc.sub[keep,] zinb$individual &lt;- colData(umi.qc.sub)$individual # we need to reorganize the assays in the SumExp from splatter nms &lt;- c(&quot;counts&quot;, setdiff(assayNames(zinb), &quot;counts&quot;)) assays(zinb) &lt;- assays(zinb)[&quot;counts&quot;] # c(&quot;counts&quot;, &quot;glm_indi&quot;)] # epsilon setting as recommended by the ZINB-WaVE integration paper # system.time({ zinb &lt;- zinbwave(zinb, K=0, BPPARAM=SerialParam(), epsilon=1e12) # }) Van den Berge and Perraudeau and others have shown the LRT may perform better for null hypothesis testing, so we use the LRT. In order to use the Wald test, it is recommended to set useT=TRUE. suppressPackageStartupMessages(library(DESeq2)) dds &lt;- DESeqDataSet(zinb, design=~individual) dds &lt;- DESeq(dds, test=&quot;LRT&quot;, reduced=~1, sfType=&quot;poscounts&quot;, minmu=1e-6, minRep=Inf) "]
]
